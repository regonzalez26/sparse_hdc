{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility operations\n",
    "from numpy import log as ln\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Saving objects\n",
    "import pickle\n",
    "\n",
    "# Optimization\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCModels():\n",
    "    @classmethod\n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class ItemMemories():\n",
    "    @classmethod\n",
    "    def save_IM(self, im, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(im, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_IM(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class SparseHDC():\n",
    "    # Cyclic shifts the input hypervector arr by shift_count\n",
    "    @classmethod\n",
    "    def cyclic_shift(self, arr, shift_count=1):\n",
    "        return np.concatenate((arr[-shift_count:],arr[:-shift_count]))\n",
    "    \n",
    "    @classmethod\n",
    "    def dot(self, hv1, hv2):\n",
    "        return np.sum(np.logical_and(hv1, hv2))\n",
    "    \n",
    "    @classmethod\n",
    "    def disp(self, hv):\n",
    "        s = math.sqrt(len(hv))\n",
    "        if (s-int(s)):\n",
    "            return \"Must be square\"\n",
    "        \n",
    "        return np.array(hv).reshape(int(s),int(s))\n",
    "\n",
    "    # Generate a random sparse HV with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HV(self, dim = 10000, sparsity=0.3):\n",
    "        percent_sparsity = int(100*sparsity)\n",
    "        return np.vectorize(SparseHDC._generation_threshold)(np.random.randint(101,size=dim), percent_sparsity)\n",
    "    \n",
    "    # Generate count number of sparse HVs with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HVs(self, count=10, dim = 10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_random_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate a sparse HV with exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        hv[random.sample(range(1,dim),int(sparsity*dim))]=1\n",
    "        return hv\n",
    "    \n",
    "    # Generate count number of sparse HV with dimension and exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HVs(self, count=10, dim=10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # PRIVATE METHODS\n",
    "    \n",
    "    # Returns 1 if num < percent_sparsity where 0<=num<=100\n",
    "    @classmethod\n",
    "    def _generation_threshold(self, num, percent_sparsity = 30):\n",
    "        return 1 if num<percent_sparsity else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISOLET():\n",
    "    def __init__ (self, train_filepath = 'isolet1+2+3+4.csv', test_filepath = 'isolet5.csv'):\n",
    "        self.train = pd.read_csv(train_filepath, header=None)\n",
    "        self.train_X = self.train[[i for i in range(0,617)]]\n",
    "        self.train_y = self.train[617]\n",
    "        self.test = pd.read_csv(test_filepath, header=None)\n",
    "        self.test_X = self.test[[i for i in range(0,617)]]\n",
    "        self.test_y = self.test[617]\n",
    "        \n",
    "class ItemMemory():\n",
    "    def __init__(self, cim, base_hvs):\n",
    "        self.cim = cim\n",
    "        self.base_hvs = base_hvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Item Memory Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCIM():\n",
    "    def __init__(self, sparsity=0.3, dim=10000, seed=None):\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "    \n",
    "    def modify_specs(self, sparsity=None, dim=None):\n",
    "        self.sparsity = sparsity if sparsity else self.sparsity\n",
    "        self.dim = dim if dim else self.dim\n",
    "\n",
    "    def generate(self, keys):\n",
    "        if self.seed is None:\n",
    "            seed = SparseHDC.generate_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "        else:\n",
    "            seed = self.seed\n",
    "        \n",
    "        tracker = pd.Series(np.copy(seed))\n",
    "        bit_step = int(np.sum(seed)/(len(keys)-1))\n",
    "        hvs = [seed]\n",
    "\n",
    "        for i in range(1,len(keys)):\n",
    "            next_hv = np.copy(hvs[i-1])\n",
    "\n",
    "            # TURN OFF K bits\n",
    "            turnoff_index = random.sample(list(tracker[tracker==1].index), bit_step)\n",
    "            tracker[turnoff_index]=-1 #Update to cannot be touched\n",
    "            next_hv[turnoff_index]=0 #Turn them off from previous hv\n",
    "\n",
    "            # TURN ON K bits\n",
    "            turnon_index = random.sample(list(tracker[tracker==0].index), bit_step)\n",
    "            tracker[turnon_index]=-1 #Update to cannot be touched\n",
    "            next_hv[turnon_index]=1 #Turn them on\n",
    "\n",
    "            hvs.append(next_hv)\n",
    "            \n",
    "        return dict(zip(keys,hvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binding Method\n",
    "    \n",
    "class MBitSignatureBinder(): #(Imani et.al. 2019)\n",
    "    def __init__(self, base_hv_count=617, level_hv_count=10, range_multiplier=10):\n",
    "        self.base_shifts = random.sample(range(0,base_hv_count*range_multiplier), base_hv_count)\n",
    "        self.level_shifts = random.sample(range(0,level_hv_count*range_multiplier), level_hv_count)\n",
    "        \n",
    "    def bind(self, base, base_no, level, level_no):\n",
    "        return ((SparseHDC.cyclic_shift(base, self.base_shifts[base_no]) + SparseHDC.cyclic_shift(level, self.level_shifts[level_no]))>1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ones: 92\n",
      "Correctness: True\n"
     ]
    }
   ],
   "source": [
    "# MBitSignatureBinder Test\n",
    "\n",
    "mbit = MBitSignatureBinder(617, 10, range_multiplier=1)\n",
    "base = SparseHDC.generate_sparse_HV(sparsity=0.1)\n",
    "level = SparseHDC.generate_sparse_HV(sparsity=0.1)\n",
    "base_shifted = np.roll(base,mbit.base_shifts[0])\n",
    "level_shifted =  np.roll(level,mbit.level_shifts[0])\n",
    "expected_bind = ((base_shifted + level_shifted)>1).astype(int)\n",
    "print(\"Number of ones: {}\".format(np.sum(expected_bind)))\n",
    "print(\"Correctness: {}\".format(np.sum(mbit.bind(base,0,level,0)==expected_bind)==10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsifying Method\n",
    "\n",
    "class ThresholdingSparsifier():\n",
    "    def __init__(self, percent_max_val=0.3, max_val=617):\n",
    "        self.percent_max_val = percent_max_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def sparsify(self, hv):\n",
    "        return np.array((hv>self.threshold())).astype(np.int)\n",
    "    \n",
    "    def threshold(self):\n",
    "        return int(self.percent_max_val*self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Algorithms\n",
    "\n",
    "## 1. Sparse Feature Encoder\n",
    "   based on feature encoding with the operation $$X = [B_1*L_1 + B_2*L_2...]$$\n",
    "\n",
    "   ### Constructor Parameters: <br />\n",
    "   <ul>\n",
    "       <li><b>cim_generator</b> : Algorithm to generator the continuous item memory level vectors <br /></li>\n",
    "       <li><b>binder</b> : Algorithm for binding two vectors <br /></li>\n",
    "       <li><b>sparsifier</b> : Algorithm to convert accumulation hypervector back to sparse vector <br /></li>\n",
    "   </ul>\n",
    "   <br />\n",
    "   Default parameters are set for the ISOLET dataset <br />\n",
    "   <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING ALGORITHMS\n",
    "\n",
    "class Sparse_FeatureEncoder():\n",
    "    def __init__(self, cim_generator, binder, sparsifier, sparsity=0.3, feature_count=617, qlevel_count=10, dim=10000):\n",
    "        self.cim = cim_generator\n",
    "        self.binder = binder\n",
    "        self.sparsifier = sparsifier\n",
    "        self.feature_count = feature_count\n",
    "        self.qlevel_count = qlevel_count\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim    \n",
    "        self.base_hvs = SparseHDC.generate_sparse_HVs(count=feature_count, sparsity=sparsity, dim=dim)\n",
    "        \n",
    "        #Setup functions\n",
    "        self.qlevels = self.quantization_levels()\n",
    "        self.setup_CIM()\n",
    "    \n",
    "    def change_sparsity(sparsity=0.3):\n",
    "        pass\n",
    "\n",
    "    def encode(self, features, return_accumulated=False):\n",
    "        if len(features)!=self.feature_count:\n",
    "            return \"Invalid number of features\"\n",
    "\n",
    "        #Quantize\n",
    "        quantized = np.vectorize(self.quantize)(features)\n",
    "        level_nos = [self.qlevels.index(q) for q in quantized]\n",
    "        \n",
    "        #Map to CIM\n",
    "        mapped_to_hvs = [self.cim[v] for v in quantized]\n",
    "        \n",
    "         # Bind and Accumulate (Summation of Base*Level)\n",
    "        accumulated_hv = np.repeat(0,self.dim)\n",
    "        for i in range(0,self.feature_count):\n",
    "             accumulated_hv += self.binder.bind(\n",
    "                                    base=self.base_hvs[i],\n",
    "                                    base_no=i, \n",
    "                                    level=mapped_to_hvs[i],\n",
    "                                    level_no=level_nos[i]\n",
    "                                )\n",
    "        \n",
    "        thresholded_hv = self.sparsifier.sparsify(accumulated_hv)\n",
    "\n",
    "        return accumulated_hv if return_accumulated else thresholded_hv\n",
    "    \n",
    "    # ENCAPSULATED DEPENDENCY METHODS\n",
    "\n",
    "    def setup_CIM(self):\n",
    "        self.cim = self.cim.generate(self.qlevels)\n",
    "\n",
    "    # ENCODING HELPERS\n",
    "    def quantization_levels(self, min_val=-1, max_val=1, precision=5):\n",
    "        step = (max_val - min_val) / (self.qlevel_count-1)\n",
    "        return list(np.arange(min_val, max_val+(0.1*step), step).round(precision))\n",
    "            \n",
    "    def quantize(self, value):\n",
    "        return min(self.qlevels, key=lambda x:abs(x-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basehv_distances(encoder):\n",
    "    dists = []\n",
    "\n",
    "    for i in range(0,len(encoder.base_hvs)):\n",
    "        for j in range(i+1,len(encoder.base_hvs)):\n",
    "                dists.append(SparseHDC.dot(encoder.base_hvs[i], encoder.base_hvs[j]))\n",
    "\n",
    "    plt.plot(np.array(dists)/int(encoder.sparsity*encoder.dim))\n",
    "    plt.title(\"Pairwise Distance Between Base HVs\")\n",
    "    plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END\n",
    "\n",
    "class HDC_Classifier():\n",
    "    def __init__(self, encoder, ACC_THR = 125, training_data=ISOLET()):\n",
    "        self.encoder = encoder\n",
    "        self.data = training_data\n",
    "        self.class_hvs = {}\n",
    "        self.training_encoded = {}\n",
    "        self.test_encoded = None\n",
    "        self.ACC_THR = ACC_THR\n",
    "\n",
    "    def train(self, save_encodes=True):      \n",
    "        # Group rows by class\n",
    "        classes = self.train_y().unique()\n",
    "        class_rows = {}\n",
    "        class_hvs = {}\n",
    "        \n",
    "        # Rows in each class\n",
    "        class_indexes = {}\n",
    "        for class_ in classes:\n",
    "            class_indexes[class_] = list(self.train_y()[self.train_y()==class_].index)\n",
    "\n",
    "        for class_ in classes:\n",
    "            class_rows[class_] = np.array(list(self.train_X().loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "        \n",
    "        encoded = {}\n",
    "        for class_ in classes:\n",
    "            print(\"Encoding... {}% \".format(round(100*class_/classes[-1],2)))\n",
    "            encoded[class_] = pd.Series(map(self.encoder.encode, class_rows[class_]))\n",
    "        if save_encodes:\n",
    "            self.training_encoded = encoded\n",
    "        \n",
    "        accumulated = np.array([np.sum(encoded[class_]) for class_ in classes])\n",
    "        class_sparsifier = ThresholdingSparsifier(percent_max_val = self.ACC_THR/240, max_val=240)\n",
    "        thresholded = pd.Series(map(class_sparsifier.sparsify, accumulated))\n",
    "        thresholded.index = range(1,27)\n",
    "        \n",
    "        self.class_hvs = dict(thresholded)\n",
    "        \n",
    "        return \"Done\"\n",
    "    \n",
    "    def test(self):\n",
    "        encoded_test = pd.Series(map(self.encoder.encode, np.array(self.test_X())))\n",
    "        predictions = pd.Series(map(self.query, encoded_test))\n",
    "        return np.sum(predictions == self.test_y())/len(self.test_y())\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def query(self, query_hv):\n",
    "        d = dict([[class_, SparseHDC.dot(class_hv, query_hv)] for class_,class_hv in self.class_hvs.items()])\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    def train_X(self):\n",
    "        return self.data.train_X\n",
    "    \n",
    "    def train_y(self):\n",
    "        return self.data.train_y\n",
    "    \n",
    "    def test_X(self):\n",
    "        return self.data.test_X\n",
    "    \n",
    "    def test_y(self):\n",
    "        return self.data.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMIZATION RESULTS\n",
    "#Encoding from 168ms to 77.2ms @10k bits\n",
    "#Training time reduced from 80mins to 9mins\n",
    "\n",
    "#-68ms for binding <br>\n",
    "# ~-2ms for removing from function\n",
    "\n",
    "# 6.0ms for quantization\n",
    "\n",
    "# 0.5ms mapping\n",
    "\n",
    "# 0.2ms for sparsification\n",
    "#   ~-0.5ms by removing from function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT\n",
    "\n",
    "### CONVERT NEXT THREE CELLS TO CODE AND RUN AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONCE\n",
    "\n",
    "isolet = ISOLET()\n",
    "\n",
    "# Rows in each class\n",
    "class_indexes = {}\n",
    "classes = range(1,27)\n",
    "for class_ in classes:\n",
    "    class_indexes[class_] = list(isolet.train_y[isolet.train_y==class_].index)\n",
    "\n",
    "# Rows for each class\n",
    "# class_rows[class_no][sample_no], class_no corresponds to A-Z but 1-26 instead\n",
    "class_rows = {}\n",
    "for class_ in classes:\n",
    "    class_rows[class_] = np.array(list(isolet.train_X.loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "    \n",
    "# 10 rows for each class\n",
    "test_class_rows = {}\n",
    "\n",
    "for class_, rows in class_rows.items():\n",
    "    test_class_rows[class_] = rows[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def setup_feature_encoder(dim=10000, sparsity=0.3, percent_max_val=0.45, M=10,\n",
    "                          range_multiplier=1, load_IM=None, seed=None):\n",
    "    cim_generator = LinearCIM(sparsity=sparsity, dim=dim, seed=seed)\n",
    "    binder = MBitSignatureBinder(level_hv_count=M, range_multiplier=range_multiplier)\n",
    "    sparsifier = ThresholdingSparsifier(percent_max_val=percent_max_val)\n",
    "    encoder = Sparse_FeatureEncoder(\n",
    "                cim_generator = cim_generator,\n",
    "                binder = binder,\n",
    "                sparsifier = sparsifier,\n",
    "                sparsity = sparsity,\n",
    "                dim = dim,\n",
    "                qlevel_count = M\n",
    "            )\n",
    "    if load_IM:\n",
    "        im = ItemMemories.load_IM(load_IM)\n",
    "        encoder.cim = im.cim\n",
    "        encoder.base_hvs = im.base_hvs\n",
    "    return encoder\n",
    "\n",
    "def interclass_dot_product(class_hvs):\n",
    "    dot_products = []\n",
    "    classes = list(class_hvs.keys())\n",
    "    for i in range(0, len(classes)):\n",
    "        for j in range(i+1, len(classes)):\n",
    "                dot_products.append( SparseHDC.dot(class_hvs[classes[i]], class_hvs[classes[j]]) )\n",
    "\n",
    "    print(\"Mean dot {}\".format(np.average(np.array(dot_products))))\n",
    "    sns.boxplot(dot_products)\n",
    "    plt.title(\"Interclass Dot Products\")\n",
    "\n",
    "def plot_encode_sparsities(encode, target_sparsity=0.3, app_title=\"\"):\n",
    "    classes = list(encode.keys())\n",
    "    dim = len(encode[classes[0]][0])\n",
    "    no_of_ones = []\n",
    "    \n",
    "    for class_ in classes:\n",
    "        for row in encode[class_]:\n",
    "            no_of_ones.append(np.sum(row))\n",
    "            \n",
    "    sns.boxplot(np.array(no_of_ones)/dim)\n",
    "    plt.title(\"Sparsity of Encoded Vectors {}\".format(app_title))\n",
    "    plt.xlabel('sample no.')\n",
    "    plt.ylabel('sparsity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding... 3.85% \n",
      "Encoding... 7.69% \n",
      "Encoding... 11.54% \n",
      "Encoding... 15.38% \n",
      "Encoding... 19.23% \n",
      "Encoding... 23.08% \n",
      "Encoding... 26.92% \n",
      "Encoding... 30.77% \n",
      "Encoding... 34.62% \n",
      "Encoding... 38.46% \n",
      "Encoding... 42.31% \n",
      "Encoding... 46.15% \n",
      "Encoding... 50.0% \n",
      "Encoding... 53.85% \n",
      "Encoding... 57.69% \n",
      "Encoding... 61.54% \n",
      "Encoding... 65.38% \n",
      "Encoding... 69.23% \n",
      "Encoding... 73.08% \n",
      "Encoding... 76.92% \n",
      "Encoding... 80.77% \n",
      "Encoding... 84.62% \n",
      "Encoding... 88.46% \n",
      "Encoding... 92.31% \n",
      "Encoding... 96.15% \n",
      "Encoding... 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHANGE PARAMETERS HERE\n",
    "\n",
    "dim = 10000\n",
    "sparsity = 0.05\n",
    "ENC_THR = 3\n",
    "ACC_THR = 61\n",
    "M = 26\n",
    "load_IM = 'im/sp-0.05/im_sp-0.05_m{}.pkl'.format(M)\n",
    "\n",
    "feature_encoder = setup_feature_encoder(dim=dim, sparsity=sparsity, percent_max_val=ENC_THR/617, M=M, load_IM = load_IM)\n",
    "classifier = HDC_Classifier(encoder=feature_encoder, ACC_THR=ACC_THR)\n",
    "classifier.train()\n",
    "# # Save models in the format \"HDC_Classifier_<CIM Method>_<Binding Method>_<Sparsifying Method>_S<sparsity>_D<dimensions>.pkl\"\n",
    "# # e.g. HDCModels.save_model(model=classifier, filename='HDC_Classifier_IM_BIND_TRESH_S0.x_Dxxxxx.pkl')\n",
    "# HDCModels.save_model(model=classifier, filename='HDC_Classifier_LCIM_MBIT_TRESH_S0.3_D10000.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXISTING MODEL\n",
    "\n",
    "# classifier = HDCModels.load_model(filename='models/HDC_Classifier12011122_LCIM_MBIT_TH_S0.1_D10000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5150737652341244"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of ones 1984.1538461538462\n",
      "St.Dev 45.01682788579631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_hv_ones = []\n",
    "for hv in list(classifier.class_hvs.values()):\n",
    "    class_hv_ones.append(np.sum(hv))\n",
    "print(\"Mean number of ones {}\".format(np.average(class_hv_ones)))\n",
    "print(\"St.Dev {}\".format(np.std(class_hv_ones)))\n",
    "sns.boxplot(class_hv_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of ones 1435.0564283424173\n",
      "St.Dev 40.64610861762299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_hv_ones = []\n",
    "for class_ in list(classifier.training_encoded.keys()):\n",
    "    for hv in classifier.training_encoded[class_]:\n",
    "        encoded_hv_ones.append(np.sum(hv))\n",
    "print(\"Mean number of ones {}\".format(np.average(np.array(encoded_hv_ones))))\n",
    "print(\"St.Dev {}\".format(np.std(encoded_hv_ones)))\n",
    "sns.boxplot(encoded_hv_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
