{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility operations\n",
    "from numpy import log as ln\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Saving objects\n",
    "import pickle\n",
    "\n",
    "# Optimization\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCModels():\n",
    "    @classmethod\n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class ItemMemories():\n",
    "    @classmethod\n",
    "    def save_IM(self, im, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(im, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_IM(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "        \n",
    "class Data():\n",
    "    @classmethod\n",
    "    def save(self, data, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class SparseHDC():\n",
    "    # Cyclic shifts the input hypervector arr by shift_count\n",
    "    @classmethod\n",
    "    def cyclic_shift(self, arr, shift_count=1):\n",
    "        return np.concatenate((arr[-shift_count:],arr[:-shift_count]))\n",
    "    \n",
    "    @classmethod\n",
    "    def dot(self, hv1, hv2):\n",
    "        return np.sum(np.logical_and(hv1, hv2))\n",
    "    \n",
    "    @classmethod\n",
    "    def disp(self, hv):\n",
    "        s = math.sqrt(len(hv))\n",
    "        if (s-int(s)):\n",
    "            return \"Must be square\"\n",
    "        \n",
    "        return np.array(hv).reshape(int(s),int(s))\n",
    "\n",
    "    # Generate a random sparse HV with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HV(self, dim = 10000, sparsity=0.3):\n",
    "        percent_sparsity = int(100*sparsity)\n",
    "        return np.vectorize(SparseHDC._generation_threshold)(np.random.randint(101,size=dim), percent_sparsity)\n",
    "    \n",
    "    # Generate count number of sparse HVs with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HVs(self, count=10, dim = 10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_random_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate a sparse HV with exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        hv[random.sample(range(0,dim),int(sparsity*dim))]=1\n",
    "        return hv\n",
    "    \n",
    "    # Generate count number of sparse HV with dimension and exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HVs(self, count=10, dim=10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate maximally sparse HV with exact dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_max_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        step = int(1/sparsity)\n",
    "        hv[[random.sample(range(n*step, (n+1)*step),1)[0] for n in range(0,int(dim/step))]]=1\n",
    "        return hv\n",
    "    \n",
    "    # PRIVATE METHODS\n",
    "    \n",
    "    # Returns 1 if num < percent_sparsity where 0<=num<=100\n",
    "    @classmethod\n",
    "    def _generation_threshold(self, num, percent_sparsity = 30):\n",
    "        return 1 if num<percent_sparsity else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISOLET():\n",
    "    def __init__ (self, train_filepath = 'isolet1+2+3+4.csv', test_filepath = 'isolet5.csv'):\n",
    "        self.train = pd.read_csv(train_filepath, header=None)\n",
    "        self.train_X = self.train[[i for i in range(0,617)]]\n",
    "        self.train_y = self.train[617]\n",
    "        self.test = pd.read_csv(test_filepath, header=None)\n",
    "        self.test_X = self.test[[i for i in range(0,617)]]\n",
    "        self.test_y = self.test[617]\n",
    "        \n",
    "class ItemMemory():\n",
    "    def __init__(self, cim, base_hvs):\n",
    "        self.cim = cim\n",
    "        self.base_hvs = base_hvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCIM():\n",
    "    def __init__(self, sparsity=0.3, dim=10000, seed=None):\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "    \n",
    "    def modify_specs(self, sparsity=None, dim=None):\n",
    "        self.sparsity = sparsity if sparsity else self.sparsity\n",
    "        self.dim = dim if dim else self.dim\n",
    "\n",
    "    def generate(self, keys, max_sparse=False):\n",
    "        if self.seed is None:\n",
    "            if max_sparse:\n",
    "                seed = SparseHDC.generate_max_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "            else:\n",
    "                seed = SparseHDC.generate_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "        else:\n",
    "            seed = self.seed\n",
    "        \n",
    "        hvs = [seed]\n",
    "        bit_step = int(np.sum(seed)/(len(keys)-1))\n",
    "        \n",
    "        if max_sparse:\n",
    "            ranges= set(range(0,np.sum(seed)))\n",
    "            range_length = int(1/self.sparsity)\n",
    "            \n",
    "            # Iterate over the seoncds key up\n",
    "            for i in range(1, len(keys)):\n",
    "                next_hv = np.copy(hvs[i-1])\n",
    "                \n",
    "                # Get random bit_step # of bit ranges\n",
    "                range_nos = set(random.sample(ranges, k=bit_step))\n",
    "                \n",
    "                # Remove these bit ranges in the tracker\n",
    "                \n",
    "                # For each range\n",
    "                for no in range_nos:\n",
    "                    l = no*range_length\n",
    "                    u = (no+1)*range_length\n",
    "                    loc = l + list(next_hv[l:u]).index(1)\n",
    "                    mov = random.sample(set(range(l,u))-{loc}, k=1)\n",
    "                    next_hv[loc] = 0\n",
    "                    next_hv[mov] = 1\n",
    "                \n",
    "                ranges -= range_nos\n",
    "                \n",
    "                hvs.append(next_hv)\n",
    "            \n",
    "        else:\n",
    "            tracker = pd.Series(np.copy(seed))\n",
    "\n",
    "            for i in range(1,len(keys)):\n",
    "                next_hv = np.copy(hvs[i-1])\n",
    "\n",
    "                # TURN OFF K bits\n",
    "                turnoff_index = random.sample(list(tracker[tracker==1].index), bit_step)\n",
    "                tracker[turnoff_index]=-1 #Update to cannot be touched\n",
    "                next_hv[turnoff_index]=0 #Turn them off from previous hv\n",
    "\n",
    "                # TURN ON K bits\n",
    "                turnon_index = random.sample(list(tracker[tracker==0].index), bit_step)\n",
    "                tracker[turnon_index]=-1 #Update to cannot be touched\n",
    "                next_hv[turnon_index]=1 #Turn them on\n",
    "\n",
    "                hvs.append(next_hv)\n",
    "\n",
    "        return dict(zip(keys,hvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsifying Method\n",
    "\n",
    "class ThresholdingSparsifier():\n",
    "    def __init__(self, percent_max_val=0.3, max_val=617):\n",
    "        self.percent_max_val = percent_max_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def sparsify(self, hv):\n",
    "        return np.array((hv>self.threshold())).astype(np.int)\n",
    "    \n",
    "    def threshold(self):\n",
    "        return int(self.percent_max_val*self.max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ThresholdingSparsifier(percent_max_val = 14/617, max_val = 617)\n",
    "s.sparsify(np.array([13,14,15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloGNEncoder():\n",
    "    def __init__(self, M, cim, sparsifier, feature_count=617, shifts=None):\n",
    "        self.M = M\n",
    "        self.qlevels = self.quantization_levels(M)\n",
    "        self.cim_generator = cim\n",
    "        self.cim = cim.generate(self.qlevels)\n",
    "        self.sparsifier = sparsifier\n",
    "        self.shifts = random.sample(range(0,3*feature_count),feature_count) if shifts is None else shifts\n",
    "        \n",
    "    def encode(self, features, return_accumulated=False, return_shifted=False):\n",
    "        # Quantize\n",
    "        quantized =  (map(self.get_level_hv, features))\n",
    "        # Get the shifted versions\n",
    "        shifted = pd.Series(map(SparseHDC.cyclic_shift, quantized, self.shifts))\n",
    "        if return_shifted:\n",
    "            return shifted\n",
    "        # Sum up the shifted versions\n",
    "        acc = np.sum(shifted)\n",
    "        # Sparsify\n",
    "        sparse = self.sparsifier.sparsify(acc)\n",
    "        return acc if return_accumulated else sparse\n",
    "        \n",
    "\n",
    "    def quantization_levels(self, M, min_val=-1, max_val=1, precision=5):\n",
    "        step = (max_val - min_val) / (M-1)\n",
    "        qlevels = list(np.arange(min_val, max_val+(0.1*step), step).round(precision))\n",
    "        return qlevels\n",
    "\n",
    "    def get_level_hv(self, value, index=False):\n",
    "        \n",
    "        # Original\n",
    "        # closest_value = min(self.qlevels, key=lambda x:abs(x-value))\n",
    "        \n",
    "        # IF ELSE CLOSEST VALUE\n",
    "        value = int(value*10000)\n",
    "        quantized_value_level = 0\n",
    "        \n",
    "        if(value>8888):\n",
    "            quantized_value_level = 1\n",
    "        elif(value>6666):\n",
    "            quantized_value_level = 2\n",
    "        elif(value>4444):\n",
    "            quantized_value_level = 3\n",
    "        elif(value>2222):\n",
    "            quantized_value_level = 4\n",
    "        elif(value>0):\n",
    "            quantized_value_level = 5\n",
    "        elif(value>-2223):\n",
    "            quantized_value_level = 6\n",
    "        elif(value>-4445):\n",
    "            quantized_value_level = 7\n",
    "        elif(value>-6667):\n",
    "            quantized_value_level = 8\n",
    "        elif(value>-8889):\n",
    "            quantized_value_level = 9\n",
    "        else:\n",
    "            quantized_value_level = 10\n",
    "        #\n",
    "        \n",
    "        closest_value = self.qlevels[10-quantized_value_level]\n",
    "        \n",
    "        if index:\n",
    "            return self.qlevels.index(closest_value)\n",
    "        else:\n",
    "            return self.cim[closest_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END\n",
    "\n",
    "class HDC_Classifier():\n",
    "    def __init__(self, encoder, ACC_THR = 125, training_data=ISOLET()):\n",
    "        self.encoder = encoder\n",
    "        self.data = training_data\n",
    "        self.class_hvs = {}\n",
    "        self.training_encoded = {}\n",
    "        self.test_encoded = None\n",
    "        self.ACC_THR = ACC_THR\n",
    "\n",
    "    def train(self, save_encodes=True, verbose=False):      \n",
    "        # Group rows by class\n",
    "        classes = self.train_y().unique()\n",
    "        class_rows = {}\n",
    "        class_hvs = {}\n",
    "        \n",
    "    # Segregate the rows into their corresponding classes\n",
    "        # Sample limit\n",
    "        # s_limit = 500\n",
    "    \n",
    "        # Get the indexes of the rows of different classes\n",
    "        class_indexes = {}\n",
    "        for class_ in classes:\n",
    "            class_indexes[class_] = list(self.train_y()[self.train_y()==class_].index)\n",
    "            #class_indexes[class_] = list(self.train_y()[0:s_limit][self.train_y()[0:s_limit]==class_].index)\n",
    "\n",
    "        # Segregated the rows\n",
    "        for class_ in classes:\n",
    "            class_rows[class_] = np.array(list(self.train_X().loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "            #class_rows[class_] = np.array(list(self.train_X()[0:s_limit].loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "\n",
    "        encoded = {}\n",
    "        for class_ in classes:\n",
    "            if verbose:\n",
    "                print(\"Encoding... {}% \".format(round(100*class_/classes[-1],2)))\n",
    "            encoded[class_] = pd.Series(map(self.encoder.encode, class_rows[class_]))\n",
    "        if save_encodes:\n",
    "            self.training_encoded = encoded\n",
    "        \n",
    "        accumulated = np.array([np.sum(encoded[class_]) for class_ in classes])\n",
    "        class_sparsifier = ThresholdingSparsifier(percent_max_val = self.ACC_THR/240, max_val=240)\n",
    "        thresholded = pd.Series(map(class_sparsifier.sparsify, accumulated))\n",
    "        thresholded.index = range(1,27)\n",
    "        \n",
    "        self.class_hvs = dict(thresholded)\n",
    "        \n",
    "        return \"Done\"\n",
    "    \n",
    "    def test(self):\n",
    "        encoded_test = pd.Series(map(self.encoder.encode, np.array(self.test_X())))\n",
    "        predictions = pd.Series(map(self.query, encoded_test))\n",
    "        return np.sum(predictions == self.test_y())/len(self.test_y())\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def query(self, query_hv):\n",
    "        d = dict([[class_, SparseHDC.dot(class_hv, query_hv)] for class_,class_hv in self.class_hvs.items()])\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    def train_X(self):\n",
    "        return self.data.train_X\n",
    "    \n",
    "    def train_y(self):\n",
    "        return self.data.train_y\n",
    "    \n",
    "    def test_X(self):\n",
    "        return self.data.test_X\n",
    "    \n",
    "    def test_y(self):\n",
    "        return self.data.test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT\n",
    "\n",
    "### CONVERT NEXT THREE CELLS TO CODE AND RUN AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONCE\n",
    "\n",
    "isolet = ISOLET()\n",
    "\n",
    "# Rows in each class\n",
    "class_indexes = {}\n",
    "classes = range(1,27)\n",
    "for class_ in classes:\n",
    "    class_indexes[class_] = list(isolet.train_y[isolet.train_y==class_].index)\n",
    "\n",
    "# Rows for each class\n",
    "# class_rows[class_no][sample_no], class_no corresponds to A-Z but 1-26 instead\n",
    "class_rows = {}\n",
    "for class_ in classes:\n",
    "    class_rows[class_] = np.array(list(isolet.train_X.loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "    \n",
    "# 10 rows for each class\n",
    "test_class_rows = {}\n",
    "\n",
    "for class_, rows in class_rows.items():\n",
    "    test_class_rows[class_] = rows[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sparsity_vs_accumulation_threshold(encoded_training_data, sparsity, interval=[0,99]):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    \n",
    "    #Accumulate each class\n",
    "    class_accumulations = [np.sum(encoded_training_data[class_]) for class_ in classes]\n",
    "    \n",
    "    for accumulation in class_accumulations:\n",
    "        sparsities = []\n",
    "        for i in range(interval[0],interval[1]+1):\n",
    "            sp = ThresholdingSparsifier(percent_max_val=i/100, max_val=240)\n",
    "            sparsities.append(np.sum(sp.sparsify(accumulation))/dim)\n",
    "        plt.plot(range(interval[0],interval[1]+1), sparsities)\n",
    "        \n",
    "    plt.title(\"Sparsity vs Percent ACC THR (Component Sparsity ~{})\".format(sparsity))\n",
    "    plt.xlabel(\"threshold (% of component count)\")\n",
    "    plt.ylabel(\"sparsity\")\n",
    "    \n",
    "def plot_encoding_sparsity_jitter(encoded_training_data, target_sparsity, ENC_THR=\"x\"):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    no_of_ones = np.array([])\n",
    "\n",
    "    for class_ in classes:\n",
    "        no_of_ones = np.append(no_of_ones, np.vectorize(np.sum)(encoded_training_data[class_]))\n",
    "        \n",
    "    sparsities = no_of_ones\n",
    "    print(\"Mean sparsity: {}\".format(np.average(sparsities)))\n",
    "    sns.boxplot(sparsities)\n",
    "    plt.title(\"Sparsity of Encoded Training Samples at ENC_THR={}\".format(ENC_THR))\n",
    "    plt.xlabel('sample no.')\n",
    "    plt.ylabel('sparsity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=5000\n",
    "sp=0.02\n",
    "ENC_THR=14\n",
    "ACC_THR=40\n",
    "M=10\n",
    "\n",
    "hologn = HoloGNEncoder(M, LinearCIM(dim=dim, sparsity=sp),\n",
    "                        ThresholdingSparsifier(percent_max_val=ENC_THR/617, max_val=617))\n",
    "\n",
    "#Override generated CIM from file\n",
    "hologn.cim = ItemMemories.load_IM('im_5k_sp0.02m10_r0.035_thr15_maxsparse.pkl').cim\n",
    "\n",
    "#Override encoding shifts with standard shifts\n",
    "hologn.shifts = Data.load('std_shifts.pkl').shifts\n",
    "\n",
    "#Override 0 shift with 1\n",
    "hologn.shifts[hologn.shifts.index(0)] = 1 #87.49\n",
    "hologn.shifts[hologn.shifts.index(10)] = 7 #87.68\n",
    "hologn.shifts[hologn.shifts.index(20)] = 65 #88.26\n",
    "hologn.shifts[hologn.shifts.index(15)] = 25 #88.45\n",
    "\n",
    "hologn.shifts[hologn.shifts.index(9)] = 1\n",
    "\n",
    "hologn.shifts[hologn.shifts.index(5)] = 2 #88.518\n",
    "hologn.shifts[hologn.shifts.index(12)] = 9 # 88.84\n",
    "\n",
    "hologn.shifts[hologn.shifts.index(11)] = 1 # 88.96\n",
    "\n",
    "hologn.shifts[hologn.shifts.index(22)] = 12 # 89.03\n",
    "\n",
    "hologn.shifts[hologn.shifts.index(57)] = 17\n",
    "\n",
    "classifier = HDC_Classifier(hologn, ACC_THR=ACC_THR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding... 3.85% \n",
      "Encoding... 7.69% \n",
      "Encoding... 11.54% \n",
      "Encoding... 15.38% \n",
      "Encoding... 19.23% \n",
      "Encoding... 23.08% \n",
      "Encoding... 26.92% \n",
      "Encoding... 30.77% \n",
      "Encoding... 34.62% \n",
      "Encoding... 38.46% \n",
      "Encoding... 42.31% \n",
      "Encoding... 46.15% \n",
      "Encoding... 50.0% \n",
      "Encoding... 53.85% \n",
      "Encoding... 57.69% \n",
      "Encoding... 61.54% \n",
      "Encoding... 65.38% \n",
      "Encoding... 69.23% \n",
      "Encoding... 73.08% \n",
      "Encoding... 76.92% \n",
      "Encoding... 80.77% \n",
      "Encoding... 84.62% \n",
      "Encoding... 88.46% \n",
      "Encoding... 92.31% \n",
      "Encoding... 96.15% \n",
      "Encoding... 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903143040410519"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.test()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Export shift values to a txt file for verilog use\n",
    "\n",
    "f = open(\"level_shifts_8903.txt\", \"w+\")\n",
    "\n",
    "for i in range(0, len(hologn.shifts)):\n",
    "    f.write(str(hologn.shifts[i]))\n",
    "    if i < len(hologn.shifts)-1:\n",
    "        f.write(\",\\n\")\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the accumulation\n",
    "accumulated = {}\n",
    "for c, vcs in classifier.training_encoded.items():\n",
    "    accumulated[c] = np.sum(vcs)\n",
    "    \n",
    "original_accumulated = accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(distance_difference):\n",
    "    if distance_difference:\n",
    "        if distance_difference > 500:\n",
    "            return 0.05\n",
    "        elif distance_difference > 400:\n",
    "            return 0.1\n",
    "        elif distance_difference > 300:\n",
    "            return 0.15\n",
    "        elif distance_difference > 200:\n",
    "            return 0.2\n",
    "        elif distance_difference > 100:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.3\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8191148171905067\n",
      "0.6786401539448365\n",
      "0.7248236048749198\n",
      "0.6946760744066709\n",
      "0.6305323925593329\n",
      "0.7164849262347659\n",
      "0.6927517639512508\n",
      "0.7087876844130853\n",
      "0.742142398973701\n",
      "0.7517639512508018\n",
      "0.7196921103271328\n",
      "0.7799871712636305\n",
      "0.7671584348941629\n",
      "0.8120590121872996\n",
      "0.8107761385503528\n",
      "0.8422065426555484\n",
      "0.8646568313021168\n",
      "0.8960872354073124\n",
      "0.8980115458627326\n",
      "0.8999358563181526\n",
      "0.8960872354073124\n",
      "0.8883899935856319\n",
      "0.8903143040410519\n",
      "0.8973701090442592\n",
      "0.9069916613213599\n",
      "0.9198203976908275\n",
      "0.9255933290570879\n",
      "0.9262347658755613\n",
      "0.9281590763309814\n",
      "0.9268762026940346\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "misclassified = None\n",
    "\n",
    "for epoch in range(1,31):\n",
    "    # print(\"Epoch {}\".format(epoch))\n",
    "    # QUERY THE TRAINING DATA\n",
    "    query_results = {}\n",
    "    alpha = 0.50\n",
    "\n",
    "    for c, vcs in classifier.training_encoded.items():\n",
    "        query_results[c] = pd.Series(map(classifier.query, vcs))\n",
    "    \n",
    "    misclassified = 0\n",
    "    # RETRAINING ADJUSTMENT\n",
    "    for class_ in classes:\n",
    "        for i in range(0, len(query_results[class_])):\n",
    "            # IF THE QUERY IS MISCLASSIFIED\n",
    "            if query_results[class_][i] != class_:\n",
    "                #print(\"alpha: {}\".format(alpha))\n",
    "                \n",
    "                #print(\"{} Misclassified {} into {}\".format(i, class_, query_results[class_][i]))\n",
    "                misclassified += 1\n",
    "                \n",
    "    #alpha = get_alpha(misclassified)\n",
    "    \n",
    "\n",
    "    # RETRAINING ADJUSTMENT\n",
    "    for class_ in classes:\n",
    "        for i in range(0, len(query_results[class_])):\n",
    "            # IF THE QUERY IS MISCLASSIFIED\n",
    "            if query_results[class_][i] != class_:\n",
    "                #print(\"alpha: {}\".format(alpha))\n",
    "\n",
    "                # ADD TO THE CORRECT (correct class = class_)\n",
    "                accumulated[class_] = accumulated[class_] + (alpha * classifier.training_encoded[class_][i])\n",
    "\n",
    "                # SUBTRACT FROM THE WRONG (wrong class = query_results[class_][i])\n",
    "                accumulated[query_results[class_][i]] = accumulated[query_results[class_][i]] - (alpha * classifier.training_encoded[class_][i])\n",
    "    \n",
    "    #print(\"\\tTotal Misclassified: {} Alpha: {}\".format(misclassified, alpha))\n",
    "    \n",
    "    # SWEEP ACC_THR\n",
    "\n",
    "    acc_thr_accuracies = {}\n",
    "\n",
    "    for acc_thr in range(40,41):\n",
    "        c_thresh = ThresholdingSparsifier(percent_max_val=acc_thr/240, max_val=240)\n",
    "        c_thresholded = pd.Series(map(c_thresh.sparsify, list(accumulated.values())))\n",
    "        c_thresholded.index = range(1,27)\n",
    "        classifier.class_hvs = dict(c_thresholded)\n",
    "\n",
    "        accuracy = classifier.test()\n",
    "        acc_thr_accuracies[acc_thr] = accuracy\n",
    "        mean_no_of_ones = np.average( [np.sum(v) for v in list(classifier.class_hvs.values())] )\n",
    "        print(\"{}\".format(accuracy))\n",
    "\n",
    "    max_acc_thr_accuracy = max(acc_thr_accuracies, key = acc_thr_accuracies.get)\n",
    "    c_thresh = ThresholdingSparsifier(percent_max_val=max_acc_thr_accuracy/240, max_val=240)\n",
    "    c_thresholded = pd.Series(map(c_thresh.sparsify, list(accumulated.values())))\n",
    "    c_thresholded.index = range(1,27)\n",
    "    classifier.class_hvs = dict(c_thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
