{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility operations\n",
    "from numpy import log as ln\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Saving objects\n",
    "import pickle\n",
    "\n",
    "# Optimization\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCModels():\n",
    "    @classmethod\n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class ItemMemories():\n",
    "    @classmethod\n",
    "    def save_IM(self, im, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(im, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_IM(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class SparseHDC():\n",
    "    # Cyclic shifts the input hypervector arr by shift_count\n",
    "    @classmethod\n",
    "    def cyclic_shift(self, arr, shift_count=1):\n",
    "        return np.concatenate((arr[-shift_count:],arr[:-shift_count]))\n",
    "    \n",
    "    @classmethod\n",
    "    def dot(self, hv1, hv2):\n",
    "        return np.sum(np.logical_and(hv1, hv2))\n",
    "    \n",
    "    @classmethod\n",
    "    def disp(self, hv):\n",
    "        s = math.sqrt(len(hv))\n",
    "        if (s-int(s)):\n",
    "            return \"Must be square\"\n",
    "        \n",
    "        return np.array(hv).reshape(int(s),int(s))\n",
    "\n",
    "    # Generate a random sparse HV with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HV(self, dim = 10000, sparsity=0.3):\n",
    "        percent_sparsity = int(100*sparsity)\n",
    "        return np.vectorize(SparseHDC._generation_threshold)(np.random.randint(101,size=dim), percent_sparsity)\n",
    "    \n",
    "    # Generate count number of sparse HVs with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HVs(self, count=10, dim = 10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_random_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate a sparse HV with exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        hv[random.sample(range(1,dim),int(sparsity*dim))]=1\n",
    "        return hv\n",
    "    \n",
    "    # Generate count number of sparse HV with dimension and exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HVs(self, count=10, dim=10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # PRIVATE METHODS\n",
    "    \n",
    "    # Returns 1 if num < percent_sparsity where 0<=num<=100\n",
    "    @classmethod\n",
    "    def _generation_threshold(self, num, percent_sparsity = 30):\n",
    "        return 1 if num<percent_sparsity else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISOLET():\n",
    "    def __init__ (self, train_filepath = 'isolet1+2+3+4.csv', test_filepath = 'isolet5.csv'):\n",
    "        self.train = pd.read_csv(train_filepath, header=None)\n",
    "        self.train_X = self.train[[i for i in range(0,617)]]\n",
    "        self.train_y = self.train[617]\n",
    "        self.test = pd.read_csv(test_filepath, header=None)\n",
    "        self.test_X = self.test[[i for i in range(0,617)]]\n",
    "        self.test_y = self.test[617]\n",
    "        \n",
    "class ItemMemory():\n",
    "    def __init__(self, cim, base_hvs):\n",
    "        self.cim = cim\n",
    "        self.base_hvs = base_hvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsifying Method\n",
    "\n",
    "class ThresholdingSparsifier():\n",
    "    def __init__(self, percent_max_val=0.3, max_val=617):\n",
    "        self.percent_max_val = percent_max_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def sparsify(self, hv):\n",
    "        return np.array((hv>self.threshold())).astype(np.int)\n",
    "    \n",
    "    def threshold(self):\n",
    "        return int(self.percent_max_val*self.max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloGNEncoder():\n",
    "    def __init__(self, dim, sparsity, M, ENC_THR, shift_range_multiplier=1, base_hvs=None):\n",
    "        self.dim = dim\n",
    "        self.sparsity = sparsity\n",
    "        self.M = M\n",
    "        self.qlevels = self.quantization_levels(M)\n",
    "        self.shifts = random.sample(range(0,int(M*shift_range_multiplier)),M) #np.arange(0,M)\n",
    "        self.base_hvs = np.array(SparseHDC.generate_sparse_HVs(dim=dim, sparsity=sparsity, count=617)) if base_hvs is None else base_hvs\n",
    "        self.sparsifier = ThresholdingSparsifier(percent_max_val = ENC_THR/617, max_val=617)\n",
    "        \n",
    "    def encode(self, features, return_acc=False):\n",
    "        shifts = np.vectorize(self.get_shift)(features)\n",
    "        shifted_base_hvs = pd.Series(map(SparseHDC.cyclic_shift, self.base_hvs, shifts))\n",
    "        acc = np.sum(shifted_base_hvs)\n",
    "        thr = self.sparsifier.sparsify(acc)\n",
    "        return acc if return_acc else thr\n",
    "\n",
    "    def quantization_levels(self, M, min_val=-1, max_val=1, precision=5):\n",
    "        step = (max_val - min_val) / (M-1)\n",
    "        qlevels = list(np.arange(min_val, max_val+(0.1*step), step).round(precision))\n",
    "        return qlevels\n",
    "\n",
    "    def get_shift(self, value):\n",
    "        closest_value = min(self.qlevels, key=lambda x:abs(x-value))\n",
    "        return self.shifts[self.qlevels.index(closest_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END\n",
    "\n",
    "class HDC_Classifier():\n",
    "    def __init__(self, encoder, ACC_THR = 125, training_data=ISOLET()):\n",
    "        self.encoder = encoder\n",
    "        self.data = training_data\n",
    "        self.class_hvs = {}\n",
    "        self.training_encoded = {}\n",
    "        self.test_encoded = None\n",
    "        self.ACC_THR = ACC_THR\n",
    "\n",
    "    def train(self, save_encodes=True):      \n",
    "        # Group rows by class\n",
    "        classes = self.train_y().unique()\n",
    "        class_rows = {}\n",
    "        class_hvs = {}\n",
    "        \n",
    "    # Segregate the rows into their corresponding classes\n",
    "        # Get the indexes of the rows of different classes\n",
    "        class_indexes = {}\n",
    "        for class_ in classes:\n",
    "            class_indexes[class_] = list(self.train_y()[self.train_y()==class_].index)\n",
    "\n",
    "        # Segregated the rows\n",
    "        for class_ in classes:\n",
    "            class_rows[class_] = np.array(list(self.train_X().loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "\n",
    "        encoded = {}\n",
    "        for class_ in classes:\n",
    "            print(\"Encoding... {}% \".format(round(100*class_/classes[-1],2)))\n",
    "            encoded[class_] = pd.Series(map(self.encoder.encode, class_rows[class_]))\n",
    "        if save_encodes:\n",
    "            self.training_encoded = encoded\n",
    "        \n",
    "        accumulated = np.array([np.sum(encoded[class_]) for class_ in classes])\n",
    "        class_sparsifier = ThresholdingSparsifier(percent_max_val = self.ACC_THR/240, max_val=240)\n",
    "        thresholded = pd.Series(map(class_sparsifier.sparsify, accumulated))\n",
    "        thresholded.index = range(1,27)\n",
    "        \n",
    "        self.class_hvs = dict(thresholded)\n",
    "        \n",
    "        return \"Done\"\n",
    "    \n",
    "    def test(self):\n",
    "        encoded_test = pd.Series(map(self.encoder.encode, np.array(self.test_X())))\n",
    "        predictions = pd.Series(map(self.query, encoded_test))\n",
    "        return np.sum(predictions == self.test_y())/len(self.test_y())\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def query(self, query_hv):\n",
    "        d = dict([[class_, SparseHDC.dot(class_hv, query_hv)] for class_,class_hv in self.class_hvs.items()])\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    def train_X(self):\n",
    "        return self.data.train_X\n",
    "    \n",
    "    def train_y(self):\n",
    "        return self.data.train_y\n",
    "    \n",
    "    def test_X(self):\n",
    "        return self.data.test_X\n",
    "    \n",
    "    def test_y(self):\n",
    "        return self.data.test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT\n",
    "\n",
    "### CONVERT NEXT THREE CELLS TO CODE AND RUN AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONCE\n",
    "\n",
    "isolet = ISOLET()\n",
    "\n",
    "# Rows in each class\n",
    "class_indexes = {}\n",
    "classes = range(1,27)\n",
    "for class_ in classes:\n",
    "    class_indexes[class_] = list(isolet.train_y[isolet.train_y==class_].index)\n",
    "\n",
    "# Rows for each class\n",
    "# class_rows[class_no][sample_no], class_no corresponds to A-Z but 1-26 instead\n",
    "class_rows = {}\n",
    "for class_ in classes:\n",
    "    class_rows[class_] = np.array(list(isolet.train_X.loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "    \n",
    "# 10 rows for each class\n",
    "test_class_rows = {}\n",
    "\n",
    "for class_, rows in class_rows.items():\n",
    "    test_class_rows[class_] = rows[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sparsity_vs_accumulation_threshold(encoded_training_data, sparsity, interval=[0,99]):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    \n",
    "    #Accumulate each class\n",
    "    class_accumulations = [np.sum(encoded_training_data[class_]) for class_ in classes]\n",
    "    \n",
    "    for accumulation in class_accumulations:\n",
    "        sparsities = []\n",
    "        for i in range(interval[0],interval[1]+1):\n",
    "            sp = ThresholdingSparsifier(percent_max_val=i/100, max_val=240)\n",
    "            sparsities.append(np.sum(sp.sparsify(accumulation))/dim)\n",
    "        plt.plot(range(interval[0],interval[1]+1), sparsities)\n",
    "        \n",
    "    plt.title(\"Sparsity vs Percent ACC THR (Component Sparsity ~{})\".format(sparsity))\n",
    "    plt.xlabel(\"threshold (% of component count)\")\n",
    "    plt.ylabel(\"sparsity\")\n",
    "    \n",
    "def plot_encoding_sparsity_jitter(encoded_training_data, target_sparsity, ENC_THR=\"x\"):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    no_of_ones = np.array([])\n",
    "\n",
    "    for class_ in classes:\n",
    "        no_of_ones = np.append(no_of_ones, np.vectorize(np.sum)(encoded_training_data[class_]))\n",
    "        \n",
    "    sparsities = no_of_ones\n",
    "    print(\"Mean sparsity: {}\".format(np.average(sparsities)))\n",
    "    sns.boxplot(sparsities)\n",
    "    plt.title(\"Sparsity of Encoded Training Samples at ENC_THR={}\".format(ENC_THR))\n",
    "    plt.xlabel('sample no.')\n",
    "    plt.ylabel('sparsity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=10000\n",
    "sp=0.30\n",
    "ENC_THR=191\n",
    "ACC_THR=82\n",
    "M=10\n",
    "\n",
    "hologn = HoloGNEncoder(dim=dim, sparsity=sp, M=M, ENC_THR=ENC_THR)\n",
    "classifier = HDC_Classifier(hologn, ACC_THR=ACC_THR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding... 3.85% \n",
      "Encoding... 7.69% \n",
      "Encoding... 11.54% \n",
      "Encoding... 15.38% \n",
      "Encoding... 19.23% \n",
      "Encoding... 23.08% \n",
      "Encoding... 26.92% \n",
      "Encoding... 30.77% \n",
      "Encoding... 34.62% \n",
      "Encoding... 38.46% \n",
      "Encoding... 42.31% \n",
      "Encoding... 46.15% \n",
      "Encoding... 50.0% \n",
      "Encoding... 53.85% \n",
      "Encoding... 57.69% \n",
      "Encoding... 61.54% \n",
      "Encoding... 65.38% \n",
      "Encoding... 69.23% \n",
      "Encoding... 73.08% \n",
      "Encoding... 76.92% \n",
      "Encoding... 80.77% \n",
      "Encoding... 84.62% \n",
      "Encoding... 88.46% \n",
      "Encoding... 92.31% \n",
      "Encoding... 96.15% \n",
      "Encoding... 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 2940.423076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DETERMINE ACC_THR\n",
    "acc_thr = ThresholdingSparsifier(percent_max_val=90/240, max_val=240)\n",
    "\n",
    "no_of_ones = []\n",
    "for class_ in list(classifier.training_encoded.keys()):\n",
    "    no_of_ones.append( np.sum(acc_thr.sparsify(np.sum(classifier.training_encoded[class_]))) )\n",
    "\n",
    "print(\"Mean {}\".format(np.average(no_of_ones)))\n",
    "sns.boxplot(no_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
