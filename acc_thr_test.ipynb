{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility operations\n",
    "from numpy import log as ln\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Saving objects\n",
    "import pickle\n",
    "\n",
    "# Optimization\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCModels():\n",
    "    @classmethod\n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class ItemMemories():\n",
    "    @classmethod\n",
    "    def save_IM(self, im, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(im, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_IM(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "        \n",
    "class Data():\n",
    "    @classmethod\n",
    "    def save(self, data, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class SparseHDC():\n",
    "    # Cyclic shifts the input hypervector arr by shift_count\n",
    "    @classmethod\n",
    "    def cyclic_shift(self, arr, shift_count=1):\n",
    "        return np.concatenate((arr[-shift_count:],arr[:-shift_count]))\n",
    "    \n",
    "    @classmethod\n",
    "    def dot(self, hv1, hv2):\n",
    "        return np.sum(np.logical_and(hv1, hv2))\n",
    "    \n",
    "    @classmethod\n",
    "    def disp(self, hv):\n",
    "        s = math.sqrt(len(hv))\n",
    "        if (s-int(s)):\n",
    "            return \"Must be square\"\n",
    "        \n",
    "        return np.array(hv).reshape(int(s),int(s))\n",
    "\n",
    "    # Generate a random sparse HV with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HV(self, dim = 10000, sparsity=0.3):\n",
    "        percent_sparsity = int(100*sparsity)\n",
    "        return np.vectorize(SparseHDC._generation_threshold)(np.random.randint(101,size=dim), percent_sparsity)\n",
    "    \n",
    "    # Generate count number of sparse HVs with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HVs(self, count=10, dim = 10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_random_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate a sparse HV with exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        hv[random.sample(range(0,dim),int(sparsity*dim))]=1\n",
    "        return hv\n",
    "    \n",
    "    # Generate count number of sparse HV with dimension and exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HVs(self, count=10, dim=10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate maximally sparse HV with exact dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_max_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        step = int(1/sparsity)\n",
    "        hv[[random.sample(range(n*step, (n+1)*step),1)[0] for n in range(0,int(dim/step))]]=1\n",
    "        return hv\n",
    "    \n",
    "    # PRIVATE METHODS\n",
    "    \n",
    "    # Returns 1 if num < percent_sparsity where 0<=num<=100\n",
    "    @classmethod\n",
    "    def _generation_threshold(self, num, percent_sparsity = 30):\n",
    "        return 1 if num<percent_sparsity else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISOLET():\n",
    "    def __init__ (self, train_filepath = 'isolet1+2+3+4.csv', test_filepath = 'isolet5.csv'):\n",
    "        self.train = pd.read_csv(train_filepath, header=None)\n",
    "        self.train_X = self.train[[i for i in range(0,617)]]\n",
    "        self.train_y = self.train[617]\n",
    "        self.test = pd.read_csv(test_filepath, header=None)\n",
    "        self.test_X = self.test[[i for i in range(0,617)]]\n",
    "        self.test_y = self.test[617]\n",
    "        \n",
    "class ItemMemory():\n",
    "    def __init__(self, cim, base_hvs):\n",
    "        self.cim = cim\n",
    "        self.base_hvs = base_hvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCIM():\n",
    "    def __init__(self, sparsity=0.3, dim=10000, seed=None):\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "    \n",
    "    def modify_specs(self, sparsity=None, dim=None):\n",
    "        self.sparsity = sparsity if sparsity else self.sparsity\n",
    "        self.dim = dim if dim else self.dim\n",
    "\n",
    "    def generate(self, keys, max_sparse=False):\n",
    "        if self.seed is None:\n",
    "            if max_sparse:\n",
    "                seed = SparseHDC.generate_max_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "            else:\n",
    "                seed = SparseHDC.generate_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "        else:\n",
    "            seed = self.seed\n",
    "        \n",
    "        hvs = [seed]\n",
    "        bit_step = int(np.sum(seed)/(len(keys)-1))\n",
    "        \n",
    "        if max_sparse:\n",
    "            ranges= set(range(0,np.sum(seed)))\n",
    "            range_length = int(1/self.sparsity)\n",
    "            \n",
    "            # Iterate over the seoncds key up\n",
    "            for i in range(1, len(keys)):\n",
    "                next_hv = np.copy(hvs[i-1])\n",
    "                \n",
    "                # Get random bit_step # of bit ranges\n",
    "                range_nos = set(random.sample(ranges, k=bit_step))\n",
    "                \n",
    "                # Remove these bit ranges in the tracker\n",
    "                \n",
    "                # For each range\n",
    "                for no in range_nos:\n",
    "                    l = no*range_length\n",
    "                    u = (no+1)*range_length\n",
    "                    loc = l + list(next_hv[l:u]).index(1)\n",
    "                    mov = random.sample(set(range(l,u))-{loc}, k=1)\n",
    "                    next_hv[loc] = 0\n",
    "                    next_hv[mov] = 1\n",
    "                \n",
    "                ranges -= range_nos\n",
    "                \n",
    "                hvs.append(next_hv)\n",
    "            \n",
    "        else:\n",
    "            tracker = pd.Series(np.copy(seed))\n",
    "\n",
    "            for i in range(1,len(keys)):\n",
    "                next_hv = np.copy(hvs[i-1])\n",
    "\n",
    "                # TURN OFF K bits\n",
    "                turnoff_index = random.sample(list(tracker[tracker==1].index), bit_step)\n",
    "                tracker[turnoff_index]=-1 #Update to cannot be touched\n",
    "                next_hv[turnoff_index]=0 #Turn them off from previous hv\n",
    "\n",
    "                # TURN ON K bits\n",
    "                turnon_index = random.sample(list(tracker[tracker==0].index), bit_step)\n",
    "                tracker[turnon_index]=-1 #Update to cannot be touched\n",
    "                next_hv[turnon_index]=1 #Turn them on\n",
    "\n",
    "                hvs.append(next_hv)\n",
    "\n",
    "        return dict(zip(keys,hvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsifying Method\n",
    "\n",
    "class ThresholdingSparsifier():\n",
    "    def __init__(self, percent_max_val=0.3, max_val=617):\n",
    "        self.percent_max_val = percent_max_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def sparsify(self, hv):\n",
    "        return np.array((hv>self.threshold())).astype(np.int)\n",
    "    \n",
    "    def threshold(self):\n",
    "        return int(self.percent_max_val*self.max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ThresholdingSparsifier(percent_max_val = 14/617, max_val = 617)\n",
    "s.sparsify(np.array([13,14,15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloGNEncoder():\n",
    "    def __init__(self, M, cim, sparsifier, feature_count=617, shifts=None):\n",
    "        self.M = M\n",
    "        self.qlevels = self.quantization_levels(M)\n",
    "        self.cim_generator = cim\n",
    "        self.cim = cim.generate(self.qlevels)\n",
    "        self.sparsifier = sparsifier\n",
    "        self.shifts = random.sample(range(0,3*feature_count),feature_count) if shifts is None else shifts\n",
    "        \n",
    "    def encode(self, features, return_accumulated=False, return_shifted=False):\n",
    "        # Quantize\n",
    "        quantized =  (map(self.get_level_hv, features))\n",
    "        # Get the shifted versions\n",
    "        shifted = pd.Series(map(SparseHDC.cyclic_shift, quantized, self.shifts))\n",
    "        if return_shifted:\n",
    "            return shifted\n",
    "        # Sum up the shifted versions\n",
    "        acc = np.sum(shifted)\n",
    "        # Sparsify\n",
    "        sparse = self.sparsifier.sparsify(acc)\n",
    "        return acc if return_accumulated else sparse\n",
    "        \n",
    "\n",
    "    def quantization_levels(self, M, min_val=-1, max_val=1, precision=5):\n",
    "        step = (max_val - min_val) / (M-1)\n",
    "        qlevels = list(np.arange(min_val, max_val+(0.1*step), step).round(precision))\n",
    "        return qlevels\n",
    "\n",
    "    def get_level_hv(self, value, index=False):\n",
    "        \n",
    "        # Original\n",
    "        # closest_value = min(self.qlevels, key=lambda x:abs(x-value))\n",
    "        \n",
    "        # IF ELSE CLOSEST VALUE\n",
    "        value = int(value*10000)\n",
    "        quantized_value_level = 0\n",
    "        \n",
    "        if(value>8888):\n",
    "            quantized_value_level = 1\n",
    "        elif(value>6666):\n",
    "            quantized_value_level = 2\n",
    "        elif(value>4444):\n",
    "            quantized_value_level = 3\n",
    "        elif(value>2222):\n",
    "            quantized_value_level = 4\n",
    "        elif(value>0):\n",
    "            quantized_value_level = 5\n",
    "        elif(value>-2223):\n",
    "            quantized_value_level = 6\n",
    "        elif(value>-4445):\n",
    "            quantized_value_level = 7\n",
    "        elif(value>-6667):\n",
    "            quantized_value_level = 8\n",
    "        elif(value>-8889):\n",
    "            quantized_value_level = 9\n",
    "        else:\n",
    "            quantized_value_level = 10\n",
    "        #\n",
    "        \n",
    "        closest_value = self.qlevels[10-quantized_value_level]\n",
    "        \n",
    "        if index:\n",
    "            return self.qlevels.index(closest_value)\n",
    "        else:\n",
    "            return self.cim[closest_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END\n",
    "\n",
    "class HDC_Classifier():\n",
    "    def __init__(self, encoder, ACC_THR = 125, training_data=ISOLET()):\n",
    "        self.encoder = encoder\n",
    "        self.data = training_data\n",
    "        self.class_hvs = {}\n",
    "        self.training_encoded = {}\n",
    "        self.test_encoded = None\n",
    "        self.ACC_THR = ACC_THR\n",
    "\n",
    "    def train(self, save_encodes=True):      \n",
    "        # Group rows by class\n",
    "        classes = self.train_y().unique()\n",
    "        class_rows = {}\n",
    "        class_hvs = {}\n",
    "        \n",
    "    # Segregate the rows into their corresponding classes\n",
    "        # Sample limit\n",
    "        # s_limit = 500\n",
    "    \n",
    "        # Get the indexes of the rows of different classes\n",
    "        class_indexes = {}\n",
    "        for class_ in classes:\n",
    "            class_indexes[class_] = list(self.train_y()[self.train_y()==class_].index)\n",
    "            #class_indexes[class_] = list(self.train_y()[0:s_limit][self.train_y()[0:s_limit]==class_].index)\n",
    "\n",
    "        # Segregated the rows\n",
    "        for class_ in classes:\n",
    "            class_rows[class_] = np.array(list(self.train_X().loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "            #class_rows[class_] = np.array(list(self.train_X()[0:s_limit].loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "\n",
    "        encoded = {}\n",
    "        for class_ in classes:\n",
    "            #print(\"Encoding... {}% \".format(round(100*class_/classes[-1],2)))\n",
    "            encoded[class_] = pd.Series(map(self.encoder.encode, class_rows[class_]))\n",
    "        if save_encodes:\n",
    "            self.training_encoded = encoded\n",
    "        \n",
    "        accumulated = np.array([np.sum(encoded[class_]) for class_ in classes])\n",
    "        class_sparsifier = ThresholdingSparsifier(percent_max_val = self.ACC_THR/240, max_val=240)\n",
    "        thresholded = pd.Series(map(class_sparsifier.sparsify, accumulated))\n",
    "        thresholded.index = range(1,27)\n",
    "        \n",
    "        self.class_hvs = dict(thresholded)\n",
    "        \n",
    "        return \"Done\"\n",
    "    \n",
    "    def test(self):\n",
    "        encoded_test = pd.Series(map(self.encoder.encode, np.array(self.test_X())))\n",
    "        predictions = pd.Series(map(self.query, encoded_test))\n",
    "        return np.sum(predictions == self.test_y()) #/len(self.test_y())\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def query(self, query_hv):\n",
    "        d = dict([[class_, SparseHDC.dot(class_hv, query_hv)] for class_,class_hv in self.class_hvs.items()])\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    def train_X(self):\n",
    "        return self.data.train_X\n",
    "    \n",
    "    def train_y(self):\n",
    "        return self.data.train_y\n",
    "    \n",
    "    def test_X(self):\n",
    "        return self.data.test_X\n",
    "    \n",
    "    def test_y(self):\n",
    "        return self.data.test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT\n",
    "\n",
    "### CONVERT NEXT THREE CELLS TO CODE AND RUN AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONCE\n",
    "\n",
    "isolet = ISOLET()\n",
    "\n",
    "# Rows in each class\n",
    "class_indexes = {}\n",
    "classes = range(1,27)\n",
    "for class_ in classes:\n",
    "    class_indexes[class_] = list(isolet.train_y[isolet.train_y==class_].index)\n",
    "\n",
    "# Rows for each class\n",
    "# class_rows[class_no][sample_no], class_no corresponds to A-Z but 1-26 instead\n",
    "class_rows = {}\n",
    "for class_ in classes:\n",
    "    class_rows[class_] = np.array(list(isolet.train_X.loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "    \n",
    "# 10 rows for each class\n",
    "test_class_rows = {}\n",
    "\n",
    "for class_, rows in class_rows.items():\n",
    "    test_class_rows[class_] = rows[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sparsity_vs_accumulation_threshold(encoded_training_data, sparsity, interval=[0,99]):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    \n",
    "    #Accumulate each class\n",
    "    class_accumulations = [np.sum(encoded_training_data[class_]) for class_ in classes]\n",
    "    \n",
    "    for accumulation in class_accumulations:\n",
    "        sparsities = []\n",
    "        for i in range(interval[0],interval[1]+1):\n",
    "            sp = ThresholdingSparsifier(percent_max_val=i/100, max_val=240)\n",
    "            sparsities.append(np.sum(sp.sparsify(accumulation))/dim)\n",
    "        plt.plot(range(interval[0],interval[1]+1), sparsities)\n",
    "        \n",
    "    plt.title(\"Sparsity vs Percent ACC THR (Component Sparsity ~{})\".format(sparsity))\n",
    "    plt.xlabel(\"threshold (% of component count)\")\n",
    "    plt.ylabel(\"sparsity\")\n",
    "    \n",
    "def plot_encoding_sparsity_jitter(encoded_training_data, target_sparsity, ENC_THR=\"x\"):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    no_of_ones = np.array([])\n",
    "\n",
    "    for class_ in classes:\n",
    "        no_of_ones = np.append(no_of_ones, np.vectorize(np.sum)(encoded_training_data[class_]))\n",
    "        \n",
    "    sparsities = no_of_ones\n",
    "    print(\"Mean sparsity: {}\".format(np.average(sparsities)))\n",
    "    sns.boxplot(sparsities)\n",
    "    plt.title(\"Sparsity of Encoded Training Samples at ENC_THR={}\".format(ENC_THR))\n",
    "    plt.xlabel('sample no.')\n",
    "    plt.ylabel('sparsity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-301-6832829446b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1559\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-297-cd1b194e7407>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mencoded_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#/len(self.test_y())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"'{type(data).__name__}' type is unordered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_iterable_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mmaybe_iterable_to_list\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \"\"\"\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-257-134b6b384343>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, features, return_accumulated, return_shifted)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mshifted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Sum up the shifted versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshifted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Sparsify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparsifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparsify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2242\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[1;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11420\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11421\u001b[0m             )\n\u001b[1;32m> 11422\u001b[1;33m         return self._reduce(\n\u001b[0m\u001b[0;32m  11423\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11424\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4234\u001b[0m                 )\n\u001b[0;32m   4235\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4236\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_reindex_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;31m# we want to transform an object array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnansum\u001b[1;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_timedelta64_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mdtype_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mthe_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m     \u001b[0mthe_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_null_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     45\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     46\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for shift_rep in range(1,100):\n",
    "    print(shift_rep)\n",
    "    dim=5000\n",
    "    sp=0.02\n",
    "    ENC_THR=14\n",
    "    ACC_THR=40\n",
    "    M=10\n",
    "\n",
    "    hologn = HoloGNEncoder(M, LinearCIM(dim=dim, sparsity=sp),\n",
    "                            ThresholdingSparsifier(percent_max_val=ENC_THR/617, max_val=617))\n",
    "\n",
    "    #Override generated CIM from file\n",
    "    hologn.cim = ItemMemories.load_IM('im_5k_sp0.02m10_r0.035_thr15_maxsparse.pkl').cim\n",
    "\n",
    "    #Override encoding shifts with standard shifts\n",
    "    hologn.shifts = Data.load('std_shifts.pkl').shifts\n",
    "\n",
    "    #Override 0 shift with 1\n",
    "    hologn.shifts[hologn.shifts.index(0)] = 1 #87.49\n",
    "    hologn.shifts[hologn.shifts.index(10)] = 7 #87.68\n",
    "    hologn.shifts[hologn.shifts.index(20)] = 65 #88.26\n",
    "    hologn.shifts[hologn.shifts.index(15)] = shift_rep\n",
    "\n",
    "\n",
    "    classifier = HDC_Classifier(hologn, ACC_THR=ACC_THR)\n",
    "\n",
    "    classifier.train()\n",
    "    \n",
    "    print(classifier.test()/1559)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding... 3.85% \n",
      "Encoding... 7.69% \n",
      "Encoding... 11.54% \n",
      "Encoding... 15.38% \n",
      "Encoding... 19.23% \n",
      "Encoding... 23.08% \n",
      "Encoding... 26.92% \n",
      "Encoding... 30.77% \n",
      "Encoding... 34.62% \n",
      "Encoding... 38.46% \n",
      "Encoding... 42.31% \n",
      "Encoding... 46.15% \n",
      "Encoding... 50.0% \n",
      "Encoding... 53.85% \n",
      "Encoding... 57.69% \n",
      "Encoding... 61.54% \n",
      "Encoding... 65.38% \n",
      "Encoding... 69.23% \n",
      "Encoding... 73.08% \n",
      "Encoding... 76.92% \n",
      "Encoding... 80.77% \n",
      "Encoding... 84.62% \n",
      "Encoding... 88.46% \n",
      "Encoding... 92.31% \n",
      "Encoding... 96.15% \n",
      "Encoding... 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8774855676715844"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.test()/1559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"inferences_py.txt\", \"w+\")\n",
    "\n",
    "for i in range(0, len(classifier.test_X())):\n",
    "    f.write(str(classifier.query(hologn.encode(classifier.test_X().iloc[i]))))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:42\t xc:16 \t py:22 \t crrct:22.0\n",
      "i:333\t xc:12 \t py:15 \t crrct:12.0\n",
      "i:652\t xc:12 \t py:15 \t crrct:15.0\n",
      "i:717\t xc:16 \t py:22 \t crrct:22.0\n",
      "i:729\t xc:2 \t py:5 \t crrct:2.0\n",
      "i:837\t xc:2 \t py:4 \t crrct:4.0\n",
      "i:1026\t xc:22 \t py:7 \t crrct:20.0\n",
      "i:1337\t xc:4 \t py:7 \t crrct:20.0\n",
      "i:1534\t xc:14 \t py:13 \t crrct:14.0\n",
      "9 mismatches\n"
     ]
    }
   ],
   "source": [
    "inf_xc = open(\"inferences_xc.txt\", \"r\")\n",
    "inf_py = open(\"inferences_py.txt\", \"r\")\n",
    "mmtches = 0\n",
    "\n",
    "for i in range(0, len(classifier.test_X())):\n",
    "    xc = int(inf_xc.readline().strip()) + 1\n",
    "    py = int(inf_py.readline().strip()) \n",
    "    if xc != py:\n",
    "        mmtches += 1\n",
    "        print(\"i:{}\\t xc:{} \\t py:{} \\t crrct:{}\".format(i, xc, py, classifier.test_y()[i]))\n",
    "        \n",
    "print(\"{} mismatches\".format(mmtches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_vals = []\n",
    "for i in range(0, len(classifier.train_X())):\n",
    "    samp_vals += list(classifier.train_X().iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([538290., 234164., 264512., 280888., 314533., 385597., 420567.,\n",
       "        453239., 437042., 520014.]),\n",
       " array([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(samp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_bin(hex_string):\n",
    "    output = \"\"\n",
    "    \n",
    "    for c in hex_string:\n",
    "        output += \"{0:04b}\".format(int(c, 16))\n",
    "        \n",
    "    return output\n",
    "\n",
    "def bin_to_hex(bin_string):\n",
    "    output = \"\"\n",
    "    temp = \"\"\n",
    "    \n",
    "    for i in range(len(bin_string)-1, -1, -1):\n",
    "        temp = bin_string[i] + temp\n",
    "        \n",
    "        if len(temp)==4:\n",
    "            output = hex(int(temp, 2))[-1] + output\n",
    "            temp = \"\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "def bin_format(bin_string, segment_length=8):\n",
    "    output = \"\"\n",
    "    ctr = 0\n",
    "    \n",
    "    for i in range(len(bin_string)-1, -1, -1):\n",
    "        output = bin_string[i] + output\n",
    "        ctr += 1\n",
    "        \n",
    "        if ctr==segment_length:\n",
    "            ctr=0\n",
    "            output = \" \" + output\n",
    "    \n",
    "    return output\n",
    "            \n",
    "\n",
    "def hv_to_str(hv):\n",
    "    return \"\".join(str(e) for e in hv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the accumulation\n",
    "accumulated = {}\n",
    "for c, vcs in classifier.training_encoded.items():\n",
    "    accumulated[c] = np.sum(vcs)\n",
    "    \n",
    "original_accumulated = accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(distance_difference):\n",
    "    if distance_difference:\n",
    "        if distance_difference > 500:\n",
    "            return 0.05\n",
    "        elif distance_difference > 400:\n",
    "            return 0.1\n",
    "        elif distance_difference > 300:\n",
    "            return 0.15\n",
    "        elif distance_difference > 200:\n",
    "            return 0.2\n",
    "        elif distance_difference > 100:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.3\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8678640153944837\n",
      "0.8794098781270044\n",
      "0.8614496472097498\n",
      "0.8505452212957024\n",
      "0.8319435535599743\n",
      "0.844772289929442\n",
      "0.8364336112892881\n",
      "0.8505452212957024\n",
      "0.8614496472097498\n",
      "0.8678640153944837\n",
      "0.8903143040410519\n",
      "0.8871071199486851\n",
      "0.8883899935856319\n",
      "0.8794098781270044\n",
      "0.8838999358563181\n",
      "0.876844130853111\n",
      "0.8928800513149455\n",
      "0.8838999358563181\n",
      "0.8960872354073124\n",
      "0.8960872354073124\n",
      "0.8877485567671585\n",
      "0.8588838999358563\n",
      "0.8896728672225785\n",
      "0.9185375240538807\n",
      "0.9166132135984606\n",
      "0.9178960872354073\n",
      "0.9114817190506735\n",
      "0.9153303399615138\n",
      "0.9146889031430404\n",
      "0.9172546504169339\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "misclassified = None\n",
    "\n",
    "for epoch in range(1,31):\n",
    "    # print(\"Epoch {}\".format(epoch))\n",
    "    # QUERY THE TRAINING DATA\n",
    "    query_results = {}\n",
    "    alpha = 0.20\n",
    "\n",
    "    for c, vcs in classifier.training_encoded.items():\n",
    "        query_results[c] = pd.Series(map(classifier.query, vcs))\n",
    "    \n",
    "    misclassified = 0\n",
    "    # RETRAINING ADJUSTMENT\n",
    "    for class_ in classes:\n",
    "        for i in range(0, len(query_results[class_])):\n",
    "            # IF THE QUERY IS MISCLASSIFIED\n",
    "            if query_results[class_][i] != class_:\n",
    "                #print(\"alpha: {}\".format(alpha))\n",
    "                \n",
    "                #print(\"{} Misclassified {} into {}\".format(i, class_, query_results[class_][i]))\n",
    "                misclassified += 1\n",
    "                \n",
    "    #alpha = get_alpha(misclassified)\n",
    "    \n",
    "\n",
    "    # RETRAINING ADJUSTMENT\n",
    "    for class_ in classes:\n",
    "        for i in range(0, len(query_results[class_])):\n",
    "            # IF THE QUERY IS MISCLASSIFIED\n",
    "            if query_results[class_][i] != class_:\n",
    "                #print(\"alpha: {}\".format(alpha))\n",
    "\n",
    "                # ADD TO THE CORRECT (correct class = class_)\n",
    "                accumulated[class_] = accumulated[class_] + (alpha * classifier.training_encoded[class_][i])\n",
    "\n",
    "                # SUBTRACT FROM THE WRONG (wrong class = query_results[class_][i])\n",
    "                accumulated[query_results[class_][i]] = accumulated[query_results[class_][i]] - (alpha * classifier.training_encoded[class_][i])\n",
    "    \n",
    "    #print(\"\\tTotal Misclassified: {} Alpha: {}\".format(misclassified, alpha))\n",
    "    \n",
    "    # SWEEP ACC_THR\n",
    "\n",
    "    acc_thr_accuracies = {}\n",
    "\n",
    "    for acc_thr in range(40,41):\n",
    "        c_thresh = ThresholdingSparsifier(percent_max_val=acc_thr/240, max_val=240)\n",
    "        c_thresholded = pd.Series(map(c_thresh.sparsify, list(accumulated.values())))\n",
    "        c_thresholded.index = range(1,27)\n",
    "        classifier.class_hvs = dict(c_thresholded)\n",
    "\n",
    "        accuracy = classifier.test()\n",
    "        acc_thr_accuracies[acc_thr] = accuracy\n",
    "        mean_no_of_ones = np.average( [np.sum(v) for v in list(classifier.class_hvs.values())] )\n",
    "        print(\"{}\".format(accuracy))\n",
    "\n",
    "    max_acc_thr_accuracy = max(acc_thr_accuracies, key = acc_thr_accuracies.get)\n",
    "    c_thresh = ThresholdingSparsifier(percent_max_val=max_acc_thr_accuracy/240, max_val=240)\n",
    "    c_thresholded = pd.Series(map(c_thresh.sparsify, list(accumulated.values())))\n",
    "    c_thresholded.index = range(1,27)\n",
    "    classifier.class_hvs = dict(c_thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00 08000032 12018823 20608000 8050b088 50050090 00002040 00050000 50000808 08001040 00104240 11180029 01000000 00001900 61a04380 80605030 04020800 05a00800 00002602 00222000 02040100 41080005 00040000 80028000 4101000c 00040620 00818310 41108010 0081a4c1 0021d000 10000000 04044000 00020800 00002200 00028400 00020401 10002000 02000400 05088c21 0a800082 24104004 20001800 000a201e 94000010 00601010 88000010 00020001 242e4a03 c0004880 c0290ac0 80040940 21420286 b0402b29 c0d62220 02240408 42164020 2c48d008 04642002 04000680 84420c08 80000002 94b20880 28480000 84004450 10206001 00082285 50110000 01400240 00420020 89000000 10008204 42000002 00008000 c0812000 00704800 00400000 48600408 02002820 04000000 00022003 04006000 01008881 26408000 8120021a 10216820 40000005 28010010 00500810 00220400 00000800 818d0206 00802481 00021400 000010a0 9c40008c 00209062 43009034 18480c00 84142002 0020001b a2108620 08200001 44040000 0200c000 8400004b a8000000 10000044 01004000 44004330 00081000 2a410800 00000180 00054011 82800193 00211402 04800001 00408000 40002020 02900001 20900022 ab007204 87a82402 4198b001 04c11002 13003000 01044900 010040a0 40012359 00130851 01044080 84912220 80085084 00060000 00500081 300400e0 c8000081 420a1000 2c320604 812a0810 20040021 10000800 02422014 00040800 00021003 144e2431 03449001 28020000 21080000 40000000 62182d10 25810820 04080c04 0800000b 00100040 10020023 08619020 4c009890'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_format( bin_to_hex( hv_to_str(hologn.encode(classifier.test_X().iloc[41]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open(\"class_hvs_xc.txt\", \"r\")\n",
    "class_hvs_xc = {}\n",
    "\n",
    "for i in range(0, 26):\n",
    "    class_hvs_xc[i+1] = f.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 1:305, \n",
      "class 2:374, 737, 945, \n",
      "class 3:311, 725, \n",
      "class 4:258, 1246, \n",
      "class 5:\n",
      "class 6:499, \n",
      "class 7:80, 169, 404, 487, \n",
      "class 8:757, \n",
      "class 9:886, \n",
      "class 10:917, \n",
      "class 11:\n",
      "class 12:299, 1083, \n",
      "class 13:332, 683, 877, 1083, \n",
      "class 14:383, 427, \n",
      "class 15:696, 745, \n",
      "class 16:488, \n",
      "class 17:\n",
      "class 18:\n",
      "class 19:928, \n",
      "class 20:816, \n",
      "class 21:461, 680, 906, 1243, \n",
      "class 22:\n",
      "class 23:765, \n",
      "class 24:136, \n",
      "class 25:\n",
      "class 26:\n"
     ]
    }
   ],
   "source": [
    "for class_ in range(1,27):\n",
    "    print(\"class {}:\".format(class_), end=\"\")\n",
    "    for i in range(0, 1250):\n",
    "        if not bin_to_hex(hv_to_str(classifier.class_hvs[class_]))[i] == class_hvs_xc[class_][i]:\n",
    "            print(i, end=\", \")\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "5 5\n",
      "6 6\n",
      "6 6\n",
      "6 6\n",
      "10 10\n",
      "10 10\n",
      "9 9\n",
      "9 9\n",
      "9 9\n",
      "9 9\n",
      "9 9\n",
      "9 9\n",
      "8 8\n",
      "8 8\n",
      "8 8\n",
      "8 8\n",
      "7 7\n",
      "7 7\n",
      "7 7\n",
      "7 7\n",
      "7 7\n",
      "6 6\n",
      "6 6\n",
      "6 6\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(test_values)):\n",
    "    print(10-hologn.get_level_hv(test_values[i], index=True), end=\"\")\n",
    "    print(\" {}\".format(x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5444\n"
     ]
    }
   ],
   "source": [
    "mid_levels = [0.8889, 0.6667, 0.4444, 0.22222, 0.0, -0.2222, -0.4444, -0.6667, -0.8889]\n",
    "\n",
    "contains_midvalues = 0\n",
    "\n",
    "for i in range(0, len(classifier.train_X())):\n",
    "    if not len(list( set(mid_levels) - set(list(classifier.train_X().iloc[i])) )) == len(mid_levels):\n",
    "        contains_midvalues += 1\n",
    "        \n",
    "print(contains_midvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "Yes\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "for m in mid_levels:\n",
    "    for c in list(classifier.train_X().iloc[1]):\n",
    "        if c == m:\n",
    "            print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in classifier.train_X().iloc[0]:\n",
    "    if i == 0.0:\n",
    "        print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4394,\n",
       " -0.093,\n",
       " 0.1718,\n",
       " 0.462,\n",
       " 0.6226,\n",
       " 0.4704,\n",
       " 0.3578,\n",
       " 0.0478,\n",
       " -0.1184,\n",
       " -0.231,\n",
       " -0.2958,\n",
       " -0.2704,\n",
       " -0.262,\n",
       " -0.217,\n",
       " -0.0874,\n",
       " -0.0564,\n",
       " 0.0254,\n",
       " 0.0958,\n",
       " 0.4226,\n",
       " 0.6648,\n",
       " 0.9184,\n",
       " 0.9718,\n",
       " 0.9324,\n",
       " 0.707,\n",
       " 0.6986,\n",
       " 0.755,\n",
       " 0.8816,\n",
       " 1.0,\n",
       " 0.938,\n",
       " 0.845,\n",
       " 0.7268,\n",
       " 0.5578,\n",
       " -0.433,\n",
       " -0.1982,\n",
       " 0.127,\n",
       " 0.3666,\n",
       " 0.4496,\n",
       " 0.4258,\n",
       " 0.2646,\n",
       " -0.0368,\n",
       " -0.07,\n",
       " -0.229,\n",
       " -0.2622,\n",
       " -0.3428,\n",
       " -0.191,\n",
       " -0.2242,\n",
       " -0.153,\n",
       " 0.0344,\n",
       " 0.108,\n",
       " 0.146,\n",
       " 0.338,\n",
       " 0.6726,\n",
       " 0.822,\n",
       " 1.0,\n",
       " 0.7912,\n",
       " 0.656,\n",
       " 0.6466,\n",
       " 0.6916,\n",
       " 0.6252,\n",
       " 0.694,\n",
       " 0.6986,\n",
       " 0.5848,\n",
       " 0.478,\n",
       " 0.3334,\n",
       " -0.3872,\n",
       " -0.129,\n",
       " 0.1656,\n",
       " 0.4394,\n",
       " 0.5228,\n",
       " 0.3534,\n",
       " 0.163,\n",
       " -0.0692,\n",
       " -0.1186,\n",
       " -0.1734,\n",
       " -0.2074,\n",
       " -0.4028,\n",
       " -0.283,\n",
       " -0.3846,\n",
       " -0.2698,\n",
       " -0.1656,\n",
       " 0.017,\n",
       " 0.0612,\n",
       " 0.1604,\n",
       " 0.5958,\n",
       " 0.7654,\n",
       " 1.0,\n",
       " 0.807,\n",
       " 0.7574,\n",
       " 0.7314,\n",
       " 0.7184,\n",
       " 0.6376,\n",
       " 0.6532,\n",
       " 0.4472,\n",
       " 0.335,\n",
       " 0.309,\n",
       " 0.0144,\n",
       " -0.3698,\n",
       " -0.0616,\n",
       " 0.2408,\n",
       " 0.5882,\n",
       " 0.6806,\n",
       " 0.196,\n",
       " 0.0252,\n",
       " 0.0644,\n",
       " -0.2212,\n",
       " -0.283,\n",
       " -0.2942,\n",
       " -0.5994,\n",
       " -0.3838,\n",
       " -0.4566,\n",
       " -0.6834,\n",
       " -0.4034,\n",
       " -0.2352,\n",
       " -0.1456,\n",
       " -0.0896,\n",
       " 0.395,\n",
       " 0.5798,\n",
       " 0.8768,\n",
       " 1.0,\n",
       " 0.9888,\n",
       " 0.8712,\n",
       " 0.9608,\n",
       " 0.9104,\n",
       " 1.0,\n",
       " 0.703,\n",
       " 0.6134,\n",
       " 0.535,\n",
       " 0.367,\n",
       " -0.2942,\n",
       " 0.0028,\n",
       " 0.308,\n",
       " 0.5178,\n",
       " 0.6458,\n",
       " 0.109,\n",
       " -0.188,\n",
       " -0.1444,\n",
       " -0.4414,\n",
       " -0.4468,\n",
       " -0.3896,\n",
       " -0.5286,\n",
       " -0.5832,\n",
       " -0.6268,\n",
       " -0.5504,\n",
       " -0.4904,\n",
       " -0.4578,\n",
       " -0.406,\n",
       " -0.2344,\n",
       " 0.079,\n",
       " 0.3324,\n",
       " 0.703,\n",
       " 0.9456,\n",
       " 1.0,\n",
       " 0.981,\n",
       " 0.9836,\n",
       " 0.9892,\n",
       " 0.91,\n",
       " 0.6758,\n",
       " 0.5994,\n",
       " 0.515,\n",
       " 0.466,\n",
       " 0.007,\n",
       " 0.584,\n",
       " 0.3812,\n",
       " 0.458,\n",
       " 0.07,\n",
       " -0.1084,\n",
       " -0.5944,\n",
       " -0.514,\n",
       " -0.6468,\n",
       " -0.8252,\n",
       " -0.8006,\n",
       " -0.6538,\n",
       " -0.6994,\n",
       " -0.7518,\n",
       " -0.4336,\n",
       " -0.5174,\n",
       " -0.4896,\n",
       " -0.493,\n",
       " -0.4896,\n",
       " -0.0034,\n",
       " 0.3146,\n",
       " 0.8216,\n",
       " 0.8426,\n",
       " 0.8812,\n",
       " 0.8916,\n",
       " 1.0,\n",
       " 0.8356,\n",
       " 0.556,\n",
       " 0.3042,\n",
       " 0.1258,\n",
       " 0.0734,\n",
       " -0.0874,\n",
       " -0.4728,\n",
       " 0.6218,\n",
       " 0.2952,\n",
       " -0.3696,\n",
       " -0.5014,\n",
       " -0.4958,\n",
       " -0.4842,\n",
       " -0.381,\n",
       " -0.639,\n",
       " -0.4498,\n",
       " -0.3008,\n",
       " -0.2952,\n",
       " -0.3352,\n",
       " -0.5014,\n",
       " -0.2608,\n",
       " -0.169,\n",
       " -0.232,\n",
       " -0.0372,\n",
       " 0.232,\n",
       " 0.49,\n",
       " 0.6446,\n",
       " 0.4958,\n",
       " 0.404,\n",
       " 0.7078,\n",
       " 1.0,\n",
       " 0.914,\n",
       " 0.5816,\n",
       " -0.318,\n",
       " -0.4098,\n",
       " -0.381,\n",
       " -0.3008,\n",
       " -0.1862,\n",
       " -0.4222,\n",
       " -0.1112,\n",
       " 0.1556,\n",
       " 0.3778,\n",
       " 0.5556,\n",
       " 0.5556,\n",
       " 0.2888,\n",
       " 0.0222,\n",
       " -0.0222,\n",
       " -0.2444,\n",
       " -0.2,\n",
       " -0.2444,\n",
       " -0.1556,\n",
       " -0.1556,\n",
       " -0.1556,\n",
       " -0.0222,\n",
       " 0.0666,\n",
       " 0.1556,\n",
       " 0.4666,\n",
       " 0.7334,\n",
       " 0.8666,\n",
       " 0.9556,\n",
       " 0.8666,\n",
       " 0.7334,\n",
       " 0.8222,\n",
       " 0.9112,\n",
       " 0.9112,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9112,\n",
       " 0.8222,\n",
       " 0.7334,\n",
       " -0.5428,\n",
       " -0.1428,\n",
       " 0.0858,\n",
       " 0.3142,\n",
       " 0.4858,\n",
       " 0.6,\n",
       " 0.1428,\n",
       " -0.1428,\n",
       " -0.3714,\n",
       " -0.4286,\n",
       " -0.8286,\n",
       " -0.6,\n",
       " -0.4858,\n",
       " -0.4286,\n",
       " -0.3714,\n",
       " -0.3714,\n",
       " -0.4286,\n",
       " -0.4858,\n",
       " -0.2,\n",
       " 0.1428,\n",
       " 0.4858,\n",
       " 0.4858,\n",
       " 0.7142,\n",
       " 0.6,\n",
       " 0.6572,\n",
       " 0.8286,\n",
       " 0.8286,\n",
       " 0.9428,\n",
       " 0.9428,\n",
       " 1.0,\n",
       " 0.8858,\n",
       " 0.7142,\n",
       " -0.2658,\n",
       " 0.0632,\n",
       " 0.2912,\n",
       " 0.2912,\n",
       " 0.1392,\n",
       " -0.038,\n",
       " -0.4936,\n",
       " -0.3924,\n",
       " -0.6962,\n",
       " -0.8482,\n",
       " -0.7974,\n",
       " -0.6962,\n",
       " -0.6202,\n",
       " -0.6456,\n",
       " -0.5444,\n",
       " -0.6456,\n",
       " -0.2912,\n",
       " -0.3924,\n",
       " -0.3164,\n",
       " 0.038,\n",
       " 0.1392,\n",
       " 0.6202,\n",
       " 0.8988,\n",
       " 1.0,\n",
       " 0.8228,\n",
       " 0.8228,\n",
       " 0.5444,\n",
       " 0.1898,\n",
       " 0.3418,\n",
       " 0.1646,\n",
       " 0.114,\n",
       " 0.114,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.895,\n",
       " -0.865,\n",
       " -0.91,\n",
       " -0.85,\n",
       " -0.385,\n",
       " 0.91,\n",
       " 0.95,\n",
       " 0.905,\n",
       " 0.85,\n",
       " 0.865,\n",
       " 0.89,\n",
       " 0.8,\n",
       " 0.51,\n",
       " 0.26,\n",
       " -0.09,\n",
       " -0.635,\n",
       " -0.89,\n",
       " -0.905,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.8842,\n",
       " -0.8092,\n",
       " -0.811,\n",
       " -0.1482,\n",
       " -0.4174,\n",
       " 0.6814,\n",
       " 0.7224,\n",
       " 0.8842,\n",
       " 0.6286,\n",
       " 0.5094,\n",
       " 0.4958,\n",
       " 0.3152,\n",
       " 0.1226,\n",
       " -0.334,\n",
       " -0.5758,\n",
       " -0.804,\n",
       " -0.9028,\n",
       " -0.925,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.76,\n",
       " -0.15,\n",
       " 0.01,\n",
       " 0.26,\n",
       " 0.1432,\n",
       " -0.525,\n",
       " -0.4218,\n",
       " -0.4718,\n",
       " -0.4384,\n",
       " -0.5518,\n",
       " -0.6484,\n",
       " -0.6186,\n",
       " -0.5216,\n",
       " -0.5294,\n",
       " -0.7448,\n",
       " -0.6278,\n",
       " -0.8534,\n",
       " -0.9268,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.3428,\n",
       " -0.4,\n",
       " -0.5714,\n",
       " -0.3714,\n",
       " 1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.9142,\n",
       " -0.9714,\n",
       " -0.9714,\n",
       " -0.9714,\n",
       " -0.5428,\n",
       " -0.5142,\n",
       " -0.8286,\n",
       " -0.1714,\n",
       " -0.6572,\n",
       " -0.5428,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9696,\n",
       " 0.5646,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9998,\n",
       " 0.9946,\n",
       " 0.9772,\n",
       " 0.9804,\n",
       " 1.0,\n",
       " 0.9264,\n",
       " -0.4934,\n",
       " 0.4942,\n",
       " -0.4122,\n",
       " 0.627,\n",
       " 0.6422,\n",
       " 0.603,\n",
       " 0.6934,\n",
       " -0.75,\n",
       " -0.4824,\n",
       " -0.5464,\n",
       " -0.3738,\n",
       " -0.3482,\n",
       " -0.1948,\n",
       " -0.1694,\n",
       " -0.1694,\n",
       " -0.099,\n",
       " -0.3418,\n",
       " -0.1374,\n",
       " 0.1502,\n",
       " 0.0798,\n",
       " 0.1758,\n",
       " 0.3802,\n",
       " 0.706,\n",
       " 0.7252,\n",
       " 0.5272,\n",
       " 0.508,\n",
       " 0.3802,\n",
       " 0.0926,\n",
       " 0.2908,\n",
       " 0.361,\n",
       " -0.0734,\n",
       " 0.016,\n",
       " -0.214,\n",
       " 0.1054,\n",
       " 0.1694,\n",
       " 0.5016,\n",
       " 0.7444,\n",
       " 0.8274,\n",
       " 1.0,\n",
       " 0.7252,\n",
       " -0.5644,\n",
       " -0.5958,\n",
       " -0.3596,\n",
       " -0.1234,\n",
       " -0.2494,\n",
       " -0.2914,\n",
       " -0.1706,\n",
       " -0.1864,\n",
       " -0.328,\n",
       " 0.0604,\n",
       " 0.4016,\n",
       " 0.4278,\n",
       " 0.1864,\n",
       " 0.5328,\n",
       " 0.6326,\n",
       " 0.6482,\n",
       " 0.3754,\n",
       " 0.3544,\n",
       " 0.5854,\n",
       " 0.1864,\n",
       " 0.8162,\n",
       " 0.6536,\n",
       " -0.1286,\n",
       " -0.0132,\n",
       " 0.0446,\n",
       " 0.2808,\n",
       " 0.4804,\n",
       " 0.7008,\n",
       " 0.7952,\n",
       " 0.9212,\n",
       " 1.0,\n",
       " 0.5224,\n",
       " -0.6836,\n",
       " -0.5,\n",
       " -0.1174,\n",
       " -0.1938,\n",
       " -0.4082,\n",
       " -0.352,\n",
       " -0.3826,\n",
       " -0.403,\n",
       " -0.1326,\n",
       " 0.4234,\n",
       " 0.9898,\n",
       " 0.9898,\n",
       " 0.7806,\n",
       " 0.7858,\n",
       " 1.0,\n",
       " 0.8674,\n",
       " 0.6836,\n",
       " 0.5408,\n",
       " 0.4796,\n",
       " 0.1224,\n",
       " 0.8164,\n",
       " 0.5816,\n",
       " -0.3572,\n",
       " -0.0408,\n",
       " 0.2448,\n",
       " 0.1582,\n",
       " 0.3316,\n",
       " 0.6888,\n",
       " 0.8572,\n",
       " 0.8622,\n",
       " 0.898,\n",
       " 0.546,\n",
       " -0.891,\n",
       " -1.0,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 0.1334,\n",
       " -1.0,\n",
       " -0.077,\n",
       " 0.0512,\n",
       " 0.2564,\n",
       " 0.5642,\n",
       " 0.4872,\n",
       " 0.077,\n",
       " 0.4358,\n",
       " 0.7436,\n",
       " 0.5128,\n",
       " 0.6666,\n",
       " 0.641,\n",
       " 0.6154,\n",
       " 1.0,\n",
       " 0.8206,\n",
       " 0.641,\n",
       " 0.359,\n",
       " 0.6924,\n",
       " 0.4358,\n",
       " 0.1538,\n",
       " 0.4616,\n",
       " 0.6154,\n",
       " 0.3334,\n",
       " 0.3334,\n",
       " 0.4102,\n",
       " 0.2052,\n",
       " 0.3846,\n",
       " 0.359,\n",
       " 0.5898,\n",
       " 0.3334,\n",
       " 0.641,\n",
       " 0.5898,\n",
       " -0.4872]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.train_X().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.88889\n",
      "-0.6666700000000001\n",
      "-0.44444500000000003\n",
      "-0.22222\n",
      "0.0\n",
      "0.22222\n",
      "0.44444500000000003\n",
      "0.6666700000000001\n",
      "0.88889\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(hologn.qlevels)-1 ):\n",
    "    print( (hologn.qlevels[i] + hologn.qlevels[i+1]) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,3] in [1,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 1 :: 61 127 176 240 284 327 406 445 491 542 \n",
      "class 2 :: 70 148 206 282 332 395 482 536 592 655 \n",
      "class 3 :: 60 124 175 245 293 341 412 455 504 558 \n",
      "class 4 :: 67 143 202 276 327 390 476 533 589 650 \n",
      "class 5 :: 68 145 206 275 325 385 475 529 587 651 \n",
      "class 6 :: 41 85 124 172 203 229 286 309 344 392 \n",
      "class 7 :: 67 137 191 261 313 367 451 501 559 620 \n",
      "class 8 :: 56 122 174 232 273 317 396 443 488 544 \n",
      "class 9 :: 47 95 132 184 215 243 310 337 376 418 \n",
      "class 10 :: 53 114 160 221 268 310 384 415 464 516 \n",
      "class 11 :: 58 120 165 229 276 320 397 429 476 529 \n",
      "class 12 :: 55 110 156 221 262 300 372 411 452 503 \n",
      "class 13 :: 55 115 164 223 269 307 376 408 449 501 \n",
      "class 14 :: 56 120 168 226 274 314 386 422 467 520 \n",
      "class 15 :: 56 109 157 214 258 300 375 415 458 508 \n",
      "class 16 :: 72 146 202 277 332 393 485 537 592 655 \n",
      "class 17 :: 56 123 173 236 285 328 401 441 484 537 \n",
      "class 18 :: 43 82 116 168 199 226 285 311 343 385 \n",
      "class 19 :: 41 87 120 166 197 225 284 307 342 387 \n",
      "class 20 :: 70 143 201 276 331 390 476 528 583 649 \n",
      "class 21 :: 59 128 179 243 293 341 415 459 503 566 \n",
      "class 22 :: 70 146 208 284 337 396 483 534 592 656 \n",
      "class 23 :: 65 134 188 249 300 346 428 475 529 585 \n",
      "class 24 :: 45 95 128 176 208 238 295 323 358 399 \n",
      "class 25 :: 50 102 133 183 221 257 322 353 389 426 \n",
      "class 26 :: 61 127 181 248 298 351 429 477 531 587 \n"
     ]
    }
   ],
   "source": [
    "samp = hologn.encode( classifier.test_X().iloc[42] )\n",
    "for h,v in classifier.class_hvs.items():\n",
    "    print(\"class {} :: \".format(h), end=\"\")\n",
    "    \n",
    "    similarity_value = 0\n",
    "    for i in range(9, -1, -1):\n",
    "        l_index = i*500\n",
    "        r_index = (i+1)*500\n",
    "        \n",
    "        similarity_value += SparseHDC.dot(v[l_index: r_index], samp[l_index: r_index])\n",
    "        \n",
    "        print(\"{} \".format(similarity_value), end=\"\")\n",
    "    \n",
    "    print(\"\\n\", end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1\n",
      "0 49\n",
      "1 90\n",
      "2 124\n",
      "3 199\n",
      "4 240\n",
      "5 281\n",
      "6 337\n",
      "7 381\n",
      "8 437\n",
      "9 501\n",
      "Class 2\n",
      "0 55\n",
      "1 102\n",
      "2 147\n",
      "3 234\n",
      "4 284\n",
      "5 334\n",
      "6 398\n",
      "7 444\n",
      "8 504\n",
      "9 569\n",
      "Class 3\n",
      "0 46\n",
      "1 82\n",
      "2 115\n",
      "3 188\n",
      "4 235\n",
      "5 277\n",
      "6 329\n",
      "7 368\n",
      "8 424\n",
      "9 486\n",
      "Class 4\n",
      "0 50\n",
      "1 92\n",
      "2 134\n",
      "3 215\n",
      "4 265\n",
      "5 311\n",
      "6 372\n",
      "7 414\n",
      "8 472\n",
      "9 533\n",
      "Class 5\n",
      "0 50\n",
      "1 93\n",
      "2 136\n",
      "3 216\n",
      "4 262\n",
      "5 311\n",
      "6 373\n",
      "7 414\n",
      "8 470\n",
      "9 535\n",
      "Class 6\n",
      "0 45\n",
      "1 80\n",
      "2 104\n",
      "3 162\n",
      "4 191\n",
      "5 220\n",
      "6 270\n",
      "7 306\n",
      "8 353\n",
      "9 401\n",
      "Class 7\n",
      "0 47\n",
      "1 86\n",
      "2 122\n",
      "3 198\n",
      "4 242\n",
      "5 283\n",
      "6 341\n",
      "7 378\n",
      "8 434\n",
      "9 494\n",
      "Class 8\n",
      "0 52\n",
      "1 94\n",
      "2 131\n",
      "3 206\n",
      "4 246\n",
      "5 284\n",
      "6 337\n",
      "7 381\n",
      "8 436\n",
      "9 499\n",
      "Class 9\n",
      "0 37\n",
      "1 72\n",
      "2 97\n",
      "3 165\n",
      "4 199\n",
      "5 234\n",
      "6 285\n",
      "7 323\n",
      "8 379\n",
      "9 437\n",
      "Class 10\n",
      "0 45\n",
      "1 83\n",
      "2 111\n",
      "3 184\n",
      "4 217\n",
      "5 256\n",
      "6 307\n",
      "7 347\n",
      "8 398\n",
      "9 454\n",
      "Class 11\n",
      "0 44\n",
      "1 84\n",
      "2 114\n",
      "3 189\n",
      "4 222\n",
      "5 260\n",
      "6 317\n",
      "7 357\n",
      "8 409\n",
      "9 469\n",
      "Class 12\n",
      "0 44\n",
      "1 76\n",
      "2 110\n",
      "3 175\n",
      "4 210\n",
      "5 248\n",
      "6 303\n",
      "7 342\n",
      "8 393\n",
      "9 453\n",
      "Class 13\n",
      "0 43\n",
      "1 82\n",
      "2 113\n",
      "3 180\n",
      "4 217\n",
      "5 254\n",
      "6 309\n",
      "7 349\n",
      "8 399\n",
      "9 456\n",
      "Class 14\n",
      "0 47\n",
      "1 82\n",
      "2 112\n",
      "3 181\n",
      "4 218\n",
      "5 257\n",
      "6 311\n",
      "7 348\n",
      "8 402\n",
      "9 462\n",
      "Class 15\n",
      "0 44\n",
      "1 82\n",
      "2 118\n",
      "3 187\n",
      "4 230\n",
      "5 267\n",
      "6 320\n",
      "7 359\n",
      "8 414\n",
      "9 474\n",
      "Class 16\n",
      "0 58\n",
      "1 101\n",
      "2 145\n",
      "3 231\n",
      "4 277\n",
      "5 320\n",
      "6 384\n",
      "7 425\n",
      "8 485\n",
      "9 549\n",
      "Class 17\n",
      "0 40\n",
      "1 76\n",
      "2 108\n",
      "3 180\n",
      "4 209\n",
      "5 245\n",
      "6 294\n",
      "7 327\n",
      "8 377\n",
      "9 437\n",
      "Class 18\n",
      "0 39\n",
      "1 69\n",
      "2 95\n",
      "3 150\n",
      "4 182\n",
      "5 214\n",
      "6 266\n",
      "7 298\n",
      "8 346\n",
      "9 397\n",
      "Class 19\n",
      "0 43\n",
      "1 75\n",
      "2 99\n",
      "3 159\n",
      "4 185\n",
      "5 210\n",
      "6 262\n",
      "7 294\n",
      "8 336\n",
      "9 377\n",
      "Class 20\n",
      "0 52\n",
      "1 93\n",
      "2 129\n",
      "3 210\n",
      "4 254\n",
      "5 297\n",
      "6 354\n",
      "7 396\n",
      "8 452\n",
      "9 516\n",
      "Class 21\n",
      "0 47\n",
      "1 86\n",
      "2 125\n",
      "3 200\n",
      "4 237\n",
      "5 275\n",
      "6 342\n",
      "7 379\n",
      "8 437\n",
      "9 495\n",
      "Class 22\n",
      "0 55\n",
      "1 104\n",
      "2 149\n",
      "3 239\n",
      "4 286\n",
      "5 337\n",
      "6 406\n",
      "7 457\n",
      "8 518\n",
      "9 586\n",
      "Class 23\n",
      "0 52\n",
      "1 94\n",
      "2 127\n",
      "3 209\n",
      "4 250\n",
      "5 290\n",
      "6 347\n",
      "7 394\n",
      "8 462\n",
      "9 532\n",
      "Class 24\n",
      "0 41\n",
      "1 77\n",
      "2 104\n",
      "3 161\n",
      "4 189\n",
      "5 219\n",
      "6 266\n",
      "7 299\n",
      "8 347\n",
      "9 398\n",
      "Class 25\n",
      "0 46\n",
      "1 84\n",
      "2 122\n",
      "3 195\n",
      "4 235\n",
      "5 272\n",
      "6 329\n",
      "7 369\n",
      "8 424\n",
      "9 484\n",
      "Class 26\n",
      "0 46\n",
      "1 88\n",
      "2 127\n",
      "3 204\n",
      "4 252\n",
      "5 294\n",
      "6 349\n",
      "7 388\n",
      "8 447\n",
      "9 507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.query( hologn.encode( classifier.train_X().iloc[42] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "569\n",
      "486\n",
      "533\n",
      "535\n",
      "401\n",
      "494\n",
      "499\n",
      "437\n",
      "454\n",
      "469\n",
      "453\n",
      "456\n",
      "462\n",
      "474\n",
      "549\n",
      "437\n",
      "397\n",
      "377\n",
      "516\n",
      "495\n",
      "586\n",
      "532\n",
      "398\n",
      "484\n",
      "507\n"
     ]
    }
   ],
   "source": [
    "for h,v in classifier.class_hvs.items():\n",
    "    print(SparseHDC.dot(v, hologn.encode( classifier.train_X().iloc[42] )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
