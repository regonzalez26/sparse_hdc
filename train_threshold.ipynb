{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility operations\n",
    "from numpy import log as ln\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Saving objects\n",
    "import pickle\n",
    "\n",
    "# Optimization\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCModels():\n",
    "    @classmethod\n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class SparseHDC():\n",
    "    # Cyclic shifts the input hypervector arr by shift_count\n",
    "    @classmethod\n",
    "    def cyclic_shift(self, arr, shift_count=1):\n",
    "        return np.concatenate((arr[-shift_count:],arr[:-shift_count]))\n",
    "    \n",
    "    @classmethod\n",
    "    def dot(self, hv1, hv2):\n",
    "        return np.sum(np.logical_and(hv1, hv2))\n",
    "    \n",
    "    @classmethod\n",
    "    def disp(self, hv):\n",
    "        s = math.sqrt(len(hv))\n",
    "        if (s-int(s)):\n",
    "            return \"Must be square\"\n",
    "        \n",
    "        return np.array(hv).reshape(int(s),int(s))\n",
    "\n",
    "    # Generate a random sparse HV with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HV(self, dim = 10000, sparsity=0.3):\n",
    "        percent_sparsity = int(100*sparsity)\n",
    "        return np.vectorize(SparseHDC._generation_threshold)(np.random.randint(101,size=dim), percent_sparsity)\n",
    "    \n",
    "    # Generate count number of sparse HVs with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HVs(self, count=10, dim = 10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_random_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate a sparse HV with exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        hv[random.sample(range(1,dim),int(sparsity*dim))]=1\n",
    "        return hv\n",
    "    \n",
    "    # Generate count number of sparse HV with dimension and exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HVs(self, count=10, dim=10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # PRIVATE METHODS\n",
    "    \n",
    "    # Returns 1 if num < percent_sparsity where 0<=num<=100\n",
    "    @classmethod\n",
    "    def _generation_threshold(self, num, percent_sparsity = 30):\n",
    "        return 1 if num<percent_sparsity else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISOLET():\n",
    "    def __init__ (self, train_filepath = 'isolet1+2+3+4.csv', test_filepath = 'isolet5.csv'):\n",
    "        self.train = pd.read_csv(train_filepath, header=None)\n",
    "        self.train_X = self.train[[i for i in range(0,617)]]\n",
    "        self.train_y = self.train[617]\n",
    "        self.test = pd.read_csv(test_filepath, header=None)\n",
    "        self.test_X = self.test[[i for i in range(0,617)]]\n",
    "        self.test_y = self.test[617]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Item Memory Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCIM():\n",
    "    def __init__(self, sparsity=0.3, dim=10000):\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim\n",
    "    \n",
    "    def modify_specs(self, sparsity=None, dim=None):\n",
    "        self.sparsity = sparsity if sparsity else self.sparsity\n",
    "        self.dim = dim if dim else self.dim\n",
    "\n",
    "    def generate(self, keys):\n",
    "        seed = SparseHDC.generate_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "        tracker = pd.Series(np.copy(seed))\n",
    "        bit_step = int(np.sum(seed)/(len(keys)-1))\n",
    "        hvs = [seed]\n",
    "\n",
    "        for i in range(1,len(keys)):\n",
    "            next_hv = np.copy(hvs[i-1])\n",
    "\n",
    "            # TURN OFF K bits\n",
    "            turnoff_index = random.sample(list(tracker[tracker==1].index), bit_step)\n",
    "            tracker[turnoff_index]=-1 #Update to cannot be touched\n",
    "            next_hv[turnoff_index]=0 #Turn them off from previous hv\n",
    "\n",
    "            # TURN ON K bits\n",
    "            turnon_index = random.sample(list(tracker[tracker==0].index), bit_step)\n",
    "            tracker[turnon_index]=-1 #Update to cannot be touched\n",
    "            next_hv[turnon_index]=1 #Turn them on\n",
    "\n",
    "            hvs.append(next_hv)\n",
    "            \n",
    "        return dict(zip(keys,hvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binding Method\n",
    "    \n",
    "class MBitSignatureBinder(): #(Imani et.al. 2019)\n",
    "    def __init__(self, base_hv_count=617, level_hv_count=10, range_multiplier=10):\n",
    "        self.base_shifts = random.sample(range(0,base_hv_count*range_multiplier), base_hv_count)\n",
    "        self.level_shifts = random.sample(range(0,level_hv_count*range_multiplier), level_hv_count)\n",
    "        \n",
    "    def bind(self, base, base_no, level, level_no):\n",
    "        return ((SparseHDC.cyclic_shift(base, self.base_shifts[base_no]) + SparseHDC.cyclic_shift(level, self.level_shifts[level_no]))>1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdditiveCDTBinder Test\n",
    "\n",
    "# acdt = AdditiveCDTBinder(sparsity=0.1)\n",
    "# A = SparseHDC.generate_sparse_HV(sparsity=0.1)\n",
    "# B = SparseHDC.generate_sparse_HV(sparsity=0.1)\n",
    "# z = np.logical_or(A,B)\n",
    "# z_tilde = np.logical_or.reduce([np.roll(z,shift) for shift in acdt.random_shifts])\n",
    "# expected_bind = np.logical_and(z,z_tilde).astype(int)\n",
    "# print(\"Number of ones: {}\".format(np.sum(expected_bind)))\n",
    "# print(\"Correctness: {}\".format(np.sum(acdt.bind(A,B) == expected_bind)==10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsifying Method\n",
    "\n",
    "class ThresholdingSparsifier():\n",
    "    def __init__(self, percent_max_val=0.3, max_val=617):\n",
    "        self.threshold = int(percent_max_val*max_val)\n",
    "    \n",
    "    def sparsify(self, hv, threshold=None):\n",
    "        self.threshold = threshold if threshold else self.threshold\n",
    "        return np.array((hv>self.threshold)).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Algorithms\n",
    "\n",
    "## 1. Sparse Feature Encoder\n",
    "   based on feature encoding with the operation $$X = [B_1*L_1 + B_2*L_2...]$$\n",
    "\n",
    "   ### Constructor Parameters: <br />\n",
    "   <ul>\n",
    "       <li><b>cim_generator</b> : Algorithm to generator the continuous item memory level vectors <br /></li>\n",
    "       <li><b>binder</b> : Algorithm for binding two vectors <br /></li>\n",
    "       <li><b>sparsifier</b> : Algorithm to convert accumulation hypervector back to sparse vector <br /></li>\n",
    "   </ul>\n",
    "   <br />\n",
    "   Default parameters are set for the ISOLET dataset <br />\n",
    "   <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #TODO: injected sparsity, implemented across all the injected algorithms\n",
    "   #TODO: convert all numpy vectorize into pandas vectorize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING ALGORITHMS\n",
    "\n",
    "class Sparse_FeatureEncoder():\n",
    "    def __init__(self, cim_generator, binder, sparsifier, sparsity=0.3, feature_count=617, qlevel_count=10, dim=10000):\n",
    "        self.cim = cim_generator\n",
    "        self.binder = binder\n",
    "        self.sparsifier = sparsifier\n",
    "        self.feature_count = feature_count\n",
    "        self.qlevel_count = qlevel_count\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim    \n",
    "        self.base_hvs = SparseHDC.generate_sparse_HVs(count=feature_count, sparsity=sparsity, dim=dim)\n",
    "        \n",
    "        #Setup functions\n",
    "        self.qlevels = self.quantization_levels()\n",
    "        self.setup_CIM()\n",
    "    \n",
    "    def change_sparsity(sparsity=0.3):\n",
    "        pass\n",
    "\n",
    "    def encode(self, features, return_accumulated=False):\n",
    "        if len(features)!=self.feature_count:\n",
    "            return \"Invalid number of features\"\n",
    "\n",
    "        #Quantize\n",
    "        quantized = np.vectorize(self.quantize)(features)\n",
    "        level_nos = [self.qlevels.index(q) for q in quantized]\n",
    "        \n",
    "        #Map to CIM\n",
    "        mapped_to_hvs = [self.cim[v] for v in quantized]\n",
    "        \n",
    "         # Bind and Accumulate (Summation of Base*Level)\n",
    "        accumulated_hv = np.repeat(0,self.dim)\n",
    "        for i in range(0,self.feature_count):\n",
    "             accumulated_hv += self.binder.bind(\n",
    "                                    base=self.base_hvs[i],\n",
    "                                    base_no=i, \n",
    "                                    level=mapped_to_hvs[i],\n",
    "                                    level_no=level_nos[i]\n",
    "                                )\n",
    "        \n",
    "        #thresholded_hv = self.sparsifier.sparsify(accumulated_hv)\n",
    "\n",
    "        return accumulated_hv #if return_accumulated else thresholded_hv\n",
    "    \n",
    "    # ENCAPSULATED DEPENDENCY METHODS\n",
    "\n",
    "    def setup_CIM(self):\n",
    "        self.cim = self.cim.generate(self.qlevels)\n",
    "\n",
    "    # ENCODING HELPERS\n",
    "    def quantization_levels(self, min_val=-1, max_val=1, precision=5):\n",
    "        step = (max_val - min_val) / (self.qlevel_count-1)\n",
    "        return list(np.arange(min_val, max_val+(0.1*step), step).round(precision))\n",
    "            \n",
    "    def quantize(self, value):\n",
    "        return min(self.qlevels, key=lambda x:abs(x-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END\n",
    "\n",
    "class HDC_Classifier():\n",
    "    def __init__(self, encoder, ACC_THR = 125, training_data=ISOLET()):\n",
    "        self.encoder = encoder\n",
    "        self.data = training_data\n",
    "        self.class_hvs = {}\n",
    "        self.training_encoded = {}\n",
    "        self.test_encoded = None\n",
    "        self.ACC_THR = ACC_THR\n",
    "\n",
    "    def train(self, save_encodes=True):      \n",
    "        # Group rows by class\n",
    "        classes = self.train_y().unique()\n",
    "        class_rows = {}\n",
    "        class_hvs = {}\n",
    "        \n",
    "        # Rows in each class\n",
    "        class_indexes = {}\n",
    "        for class_ in classes:\n",
    "            class_indexes[class_] = list(self.train_y()[self.train_y()==class_].index)\n",
    "\n",
    "        for class_ in classes:\n",
    "            class_rows[class_] = np.array(list(self.train_X().loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "        \n",
    "        encoded = {}\n",
    "        for class_ in classes:\n",
    "            print(\"Encoding... {}% \".format(round(100*class_/classes[-1],2)))\n",
    "            encoded[class_] = pd.Series(map(self.encoder.encode, class_rows[class_]))\n",
    "        if save_encodes:\n",
    "            self.training_encoded = encoded\n",
    "        \n",
    "        accumulated = np.array([np.sum(encoded[class_]) for class_ in classes])\n",
    "        class_sparsifier = ThresholdingSparsifier(percent_max_val = self.ACC_THR/240, max_val=240)\n",
    "        thresholded = pd.Series(map(class_sparsifier.sparsify, accumulated))\n",
    "        thresholded.index = range(1,27)\n",
    "        \n",
    "        self.class_hvs = dict(thresholded)\n",
    "        \n",
    "        return \"Done\"\n",
    "    \n",
    "    def test(self):\n",
    "        encoded_test = pd.Series(map(self.encoder.encode, np.array(self.test_X())))\n",
    "        predictions = pd.Series(map(self.query, encoded_test))\n",
    "        return np.sum(predictions == self.test_y())/len(self.test_y())\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def query(self, query_hv):\n",
    "        d = dict([[class_, SparseHDC.dot(class_hv, query_hv)] for class_,class_hv in self.class_hvs.items()])\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    def train_X(self):\n",
    "        return self.data.train_X\n",
    "    \n",
    "    def train_y(self):\n",
    "        return self.data.train_y\n",
    "    \n",
    "    def test_X(self):\n",
    "        return self.data.test_X\n",
    "    \n",
    "    def test_y(self):\n",
    "        return self.data.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMIZATION RESULTS\n",
    "#Encoding from 168ms to 77.2ms @10k bits\n",
    "#Training time reduced from 80mins to 9mins\n",
    "\n",
    "#-68ms for binding <br>\n",
    "# ~-2ms for removing from function\n",
    "\n",
    "# 6.0ms for quantization\n",
    "\n",
    "# 0.5ms mapping\n",
    "\n",
    "# 0.2ms for sparsification\n",
    "#   ~-0.5ms by removing from function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FUNCTIONS\n",
    "\n",
    "def setup_feature_encoder(dim=10000, sparsity=0.3, percent_max_val=0.45, M=10, range_multiplier=1):\n",
    "    cim_generator = LinearCIM(sparsity=sparsity, dim=dim)\n",
    "    binder = MBitSignatureBinder(level_hv_count=M, range_multiplier=range_multiplier)\n",
    "    sparsifier = ThresholdingSparsifier(percent_max_val=percent_max_val)\n",
    "    return Sparse_FeatureEncoder(\n",
    "                cim_generator = cim_generator,\n",
    "                binder = binder,\n",
    "                sparsifier = sparsifier,\n",
    "                sparsity = sparsity,\n",
    "                dim = dim,\n",
    "                qlevel_count = M\n",
    "            )\n",
    "\n",
    "def get_encoded_training_data(encoder):\n",
    "    classifier = HDC_Classifier(encoder = encoder)\n",
    "    classifier.train()\n",
    "    return classifier.training_encoded\n",
    "\n",
    "def get_existing_encoded_training_data(filepath):\n",
    "    return HDCModels.load_model(filepath).training_encoded\n",
    "\n",
    "def plot_encoding_sparsity_jitter(encoded_training_data, target_sparsity, ENC_THR=\"x\"):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    no_of_ones = np.array([])\n",
    "\n",
    "    for class_ in classes:\n",
    "        no_of_ones = np.append(no_of_ones, np.vectorize(np.sum)(encoded_training_data[class_]))\n",
    "        \n",
    "    sparsities = no_of_ones/dim\n",
    "    print(\"Mean sparsity: {}\".format(np.average(sparsities)))\n",
    "    sns.boxplot(sparsities)\n",
    "    plt.title(\"Sparsity of Encoded Training Samples at ENC_THR={}\".format(ENC_THR))\n",
    "    plt.xlabel('sample no.')\n",
    "    plt.ylabel('sparsity')\n",
    "\n",
    "def plot_sparsity_vs_accumulation_threshold(encoded_training_data, sparsity, interval=[0,99]):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    \n",
    "    #Accumulate each class\n",
    "    class_accumulations = [np.sum(encoded_training_data[class_]) for class_ in classes]\n",
    "    \n",
    "    for accumulation in class_accumulations:\n",
    "        sparsities = []\n",
    "        for i in range(interval[0],interval[1]+1):\n",
    "            sp = ThresholdingSparsifier(percent_max_val=i/100, max_val=240)\n",
    "            sparsities.append(np.sum(sp.sparsify(accumulation))/dim)\n",
    "        plt.plot(range(interval[0],interval[1]+1), sparsities)\n",
    "        \n",
    "    plt.title(\"Sparsity vs Percent ACC THR (Component Sparsity ~{})\".format(sparsity))\n",
    "    plt.xlabel(\"threshold (% of component count)\")\n",
    "    plt.ylabel(\"sparsity\")\n",
    "    \n",
    "def plot_classhv_sparsity_jitter(encoded_training_data, target_sparsity, ACC_THR):\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    dim = len(encoded_training_data[classes[0]][0])\n",
    "    \n",
    "    sp = ThresholdingSparsifier(percent_max_val=ACC_THR/240, max_val=240)\n",
    "    class_hvs = [sp.sparsify(np.sum(encoded_training_data[class_])) for class_ in classes]\n",
    "    sparsities = [np.sum(hv)/dim for hv in class_hvs]\n",
    "    \n",
    "    print(\"Mean sparsity: {}\".format(np.average(np.array(sparsities))))\n",
    "    sns.boxplot(sparsities)\n",
    "    plt.title(\"Sparsity of Class HVs at ACC_THR={}\".format(ACC_THR))\n",
    "    plt.xlabel('class')\n",
    "    plt.ylabel('sparsity')\n",
    "    \n",
    "def plot_interclass(encoded_training_data, wrt=1):\n",
    "    dots = []\n",
    "    classes = list(encoded_training_data.keys())\n",
    "    classes.remove(wrt)\n",
    "    for class_ in classes:\n",
    "        for i in range(0,200):\n",
    "            dots.append( SparseHDC.dot(encoded_training_data[wrt][i], encoded_training_data[class_][i]))\n",
    "    print(\"Mean dot: {}\".format(np.average(np.array(dots))))\n",
    "    plt.plot(dots)\n",
    "    plt.title(\"Dot Product Profile Comparing Class A to Other Classes\")\n",
    "    plt.xlabel('comparison no.')\n",
    "    plt.ylabel('dot product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN TESTS HERE\n",
    "### Please run the setup cells before testing as well as the library class (all the previous cells)\n",
    "## For 240 encode HVs, what must be the ACC_THR to yield class HVs with X% sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding... 3.85% \n",
      "Encoding... 7.69% \n",
      "Encoding... 11.54% \n",
      "Encoding... 15.38% \n",
      "Encoding... 19.23% \n",
      "Encoding... 23.08% \n",
      "Encoding... 26.92% \n",
      "Encoding... 30.77% \n",
      "Encoding... 34.62% \n",
      "Encoding... 38.46% \n",
      "Encoding... 42.31% \n",
      "Encoding... 46.15% \n",
      "Encoding... 50.0% \n",
      "Encoding... 53.85% \n",
      "Encoding... 57.69% \n",
      "Encoding... 61.54% \n",
      "Encoding... 65.38% \n",
      "Encoding... 69.23% \n",
      "Encoding... 73.08% \n",
      "Encoding... 76.92% \n",
      "Encoding... 80.77% \n",
      "Encoding... 84.62% \n",
      "Encoding... 88.46% \n",
      "Encoding... 92.31% \n",
      "Encoding... 96.15% \n",
      "Encoding... 100.0% \n"
     ]
    }
   ],
   "source": [
    "dim=10000\n",
    "sparsity=0.01\n",
    "ENC_THR = 6\n",
    "M=10\n",
    "\n",
    "encoder = setup_feature_encoder(dim=10000, sparsity=sparsity, percent_max_val=ENC_THR/617, M=M)\n",
    "encoded_training_data = get_encoded_training_data(encoder)\n",
    "# encoded_training_data = get_existing_encoded_training_data('HDC_Classifier2_LCIM_BIND_TRESH_S0.1_D10000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no_of_ones 99.8076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_hvs = {}\n",
    "sparsifier = ThresholdingSparsifier(percent_max_val=295/148080, max_val=148080)\n",
    "for class_ in list(encoded_training_data.keys()):\n",
    "    class_hvs[class_] = sparsifier.sparsify( np.sum(encoded_training_data[class_]) )\n",
    "\n",
    "no_of_ones = [np.sum(hv) for c,hv in class_hvs.items()]\n",
    "print(\"Mean no_of_ones {}\".format(np.average(no_of_ones)) )\n",
    "sns.boxplot(no_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean interclass dots 58.57230769230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "interclass_dots = []\n",
    "for i in range(1,27):\n",
    "    for j in range(1,27):\n",
    "        if i!=j:\n",
    "            interclass_dots.append( SparseHDC.dot(class_hvs[i], class_hvs[j]) )\n",
    "\n",
    "print(\"Mean interclass dots {}\".format(np.average(interclass_dots)))\n",
    "sns.boxplot(interclass_dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
