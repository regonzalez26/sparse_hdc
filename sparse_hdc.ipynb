{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Utility operations\n",
    "from numpy import log as ln\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "# Saving objects\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDCModels():\n",
    "    @classmethod\n",
    "    def save_model(self, model, filename):\n",
    "        with open(filename, 'wb') as outp:\n",
    "            pickle.dump(model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "class SparseHDC():\n",
    "    # Cyclic shifts the input hypervector arr by shift_count\n",
    "    @classmethod\n",
    "    def cyclic_shift(self, arr, shift_count=1):\n",
    "        return np.roll(arr, shift_count)\n",
    "    \n",
    "    @classmethod\n",
    "    def dot(self, hv1, hv2):\n",
    "        return np.sum(np.logical_and(hv1, hv2))\n",
    "    \n",
    "    @classmethod\n",
    "    def disp(self, hv):\n",
    "        s = math.sqrt(len(hv))\n",
    "        if (s-int(s)):\n",
    "            return \"Must be square\"\n",
    "        \n",
    "        return np.array(hv).reshape(int(s),int(s))\n",
    "\n",
    "    # Generate a random sparse HV with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HV(self, dim = 10000, sparsity=0.3):\n",
    "        percent_sparsity = int(100*sparsity)\n",
    "        return np.vectorize(SparseHDC._generation_threshold)(np.random.randint(101,size=dim), percent_sparsity)\n",
    "    \n",
    "    # Generate count number of sparse HVs with dimension and sparsity\n",
    "    @classmethod\n",
    "    def generate_random_sparse_HVs(self, count=10, dim = 10000, sparsity=0.3):\n",
    "        return [SparseHDC.generate_random_sparse_HV(dim, sparsity) for i in range(0,count)]\n",
    "    \n",
    "    # Generate a sparse HV with exact sparsity\n",
    "    @classmethod\n",
    "    def generate_sparse_HV(self, dim=10000, sparsity=0.3):\n",
    "        hv = np.repeat(0,dim)\n",
    "        hv[random.sample(range(1,dim),int(sparsity*dim))]=1\n",
    "        return hv\n",
    "    \n",
    "    # PRIVATE METHODS\n",
    "    \n",
    "    # Returns 1 if num < percent_sparsity where 0<=num<=100\n",
    "    @classmethod\n",
    "    def _generation_threshold(self, num, percent_sparsity = 30):\n",
    "        return 1 if num<percent_sparsity else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISOLET():\n",
    "    def __init__ (self, train_filepath = 'isolet1+2+3+4.csv', test_filepath = 'isolet5.csv'):\n",
    "        self.train = pd.read_csv(train_filepath, header=None)\n",
    "        self.train_X = self.train[[i for i in range(0,617)]]\n",
    "        self.train_y = self.train[617]\n",
    "        self.test = pd.read_csv(test_filepath, header=None)\n",
    "        self.test_X = self.test[[i for i in range(0,617)]]\n",
    "        self.test_y = self.test[617]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Item Memory Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCIM():\n",
    "    def __init__(self, sparsity=0.3, dim=10000):\n",
    "        self.sparsity = sparsity\n",
    "        self.dim = dim\n",
    "    \n",
    "    def modify_specs(self, sparsity=None, dim=None):\n",
    "        self.sparsity = sparsity if sparsity else self.sparsity\n",
    "        self.dim = dim if dim else self.dim\n",
    "\n",
    "    def generate(self, keys):\n",
    "        seed = SparseHDC.generate_sparse_HV(sparsity=self.sparsity, dim=self.dim)\n",
    "        tracker = pd.Series(np.copy(seed))\n",
    "        bit_step = int(np.sum(seed)/(len(keys)-1))\n",
    "        hvs = [seed]\n",
    "\n",
    "        for i in range(1,len(keys)):\n",
    "            next_hv = np.copy(hvs[i-1])\n",
    "\n",
    "            # TURN OFF K bits\n",
    "            turnoff_index = random.sample(list(tracker[tracker==1].index), bit_step)\n",
    "            tracker[turnoff_index]=-1 #Update to cannot be touched\n",
    "            next_hv[turnoff_index]=0 #Turn them off from previous hv\n",
    "\n",
    "            # TURN ON K bits\n",
    "            turnon_index = random.sample(list(tracker[tracker==0].index), bit_step)\n",
    "            tracker[turnon_index]=-1 #Update to cannot be touched\n",
    "            next_hv[turnon_index]=1 #Turn them on\n",
    "\n",
    "            hvs.append(next_hv)\n",
    "            \n",
    "        return dict(zip(keys,hvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binding Methods\n",
    "\n",
    "# Shortened binding time from 2.09ms to 282us by replacing np.vectorize(int) to .astype(np.int)\n",
    "# Further reducation to 194us by limiting to binding by two\n",
    "class AdditiveCDTBinder(): #(RachKovskij & Kussul, 2001)\n",
    "    def __init__(self, sparsity=0.3, component_count=2):\n",
    "        self.sparsity = sparsity\n",
    "        self.component_count = component_count\n",
    "        self.K = math.ceil(ln(1-(1/self.component_count))/ln(1-(self.sparsity*self.component_count)))\n",
    "\n",
    "    def bind(self, A,B):\n",
    "        # Disjunction of all components\n",
    "        z = np.logical_or(A,B)\n",
    "        \n",
    "        return np.logical_and( z , np.roll(z,1) ).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmented Permutation\n",
    "#segment the HV into segment_count segments, then iterate through each segment of A, make the indeces into an L length array\n",
    "#then, iterate through each segment of B, shift the segment (np.roll) by the index number.\n",
    "#append to the resultant vector as we go.\n",
    "\n",
    "#make sure dimensionality D is divisible by segment count\n",
    "class SegmentedPermutationBind():\n",
    "    def __init__(self, segment_count = 10):\n",
    "        self.segment_count = segment_count\n",
    "        \n",
    "    def bind(self, A,B):\n",
    "        z = []\n",
    "        for x in range(segment_count):\n",
    "            z = np.append(z,np.roll(B[x*(len(B)//segment_count):x*(len(B)//segment_count)+(len(B)//segment_count)],np.argmax(A[x*(len(B)//segment_count):x*(len(B)//segment_count)+(len(B)//segment_count)])))\n",
    "            \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsifying Method\n",
    "\n",
    "class ThresholdingSparsifier():\n",
    "    def __init__(self, percent_max_val=0.3, max_val=617):\n",
    "        self.threshold = int(percent_max_val*max_val)\n",
    "    \n",
    "    def sparsify(self, hv, threshold=None):\n",
    "        self.threshold = threshold if threshold else self.threshold\n",
    "        return np.array((hv>self.threshold)).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Algorithms\n",
    "\n",
    "## 1. Sparse Feature Encoder\n",
    "   based on feature encoding with the operation $$X = [B_1*L_1 + B_2*L_2...]$$\n",
    "\n",
    "   ### Constructor Parameters: <br />\n",
    "   <ul>\n",
    "       <li><b>cim_generator</b> : Algorithm to generator the continuous item memory level vectors <br /></li>\n",
    "       <li><b>binder</b> : Algorithm for binding two vectors <br /></li>\n",
    "       <li><b>sparsifier</b> : Algorithm to convert accumulation hypervector back to sparse vector <br /></li>\n",
    "   </ul>\n",
    "   <br />\n",
    "   Default parameters are set for the ISOLET dataset <br />\n",
    "   <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #TODO: injected sparsity, implemented across all the injected algorithms\n",
    "   #TODO: convert all numpy vectorize into pandas vectorize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING ALGORITHMS\n",
    "\n",
    "class Sparse_FeatureEncoder():\n",
    "    def __init__(self, cim_generator, binder, sparsifier, sparsity=0.3, feature_count=617, qlevel_count=10, dim=10000):\n",
    "        self.cim = cim_generator\n",
    "        self.binder = binder\n",
    "        self.sparsifier = sparsifier\n",
    "        self.feature_count = feature_count\n",
    "        self.qlevel_count = qlevel_count\n",
    "        self.dim = dim    \n",
    "        self.base_hvs = SparseHDC.generate_random_sparse_HVs(count=feature_count, sparsity=sparsity, dim=dim)\n",
    "        \n",
    "        #Setup functions\n",
    "        self.qlevels = self.quantization_levels()\n",
    "        self.setup_CIM()\n",
    "    \n",
    "    def change_sparsity(sparsity=0.3):\n",
    "        pass\n",
    "\n",
    "    def encode(self, features, return_accumulated=False):\n",
    "        if len(features)!=self.feature_count:\n",
    "            return \"Invalid number of features\"\n",
    "\n",
    "        #Quantize\n",
    "        quantized = np.vectorize(self.quantize)(features)\n",
    "        \n",
    "        #Map to CIM\n",
    "        mapped_to_hvs = [self.cim[v] for v in quantized]\n",
    "        \n",
    "         # Bind and Accumulate (Summation of Base*Level)\n",
    "        accumulated_hv = np.repeat(0,self.dim)\n",
    "        for i in range(0,self.feature_count):\n",
    "             accumulated_hv += self.binder.bind(self.base_hvs[i], mapped_to_hvs[i])\n",
    "        \n",
    "        thresholded_hv = self.sparsifier.sparsify(accumulated_hv)\n",
    "\n",
    "        return accumulated_hv if return_accumulated else thresholded_hv\n",
    "    \n",
    "    # ENCAPSULATED DEPENDENCY METHODS\n",
    "\n",
    "    def setup_CIM(self):\n",
    "        self.cim = self.cim.generate(self.qlevels)\n",
    "\n",
    "    # ENCODING HELPERS\n",
    "    def quantization_levels(self, min_val=-1, max_val=1, precision=5):\n",
    "        step = (max_val - min_val) / self.qlevel_count\n",
    "        return np.arange(min_val, max_val+step, step).round(precision)\n",
    "            \n",
    "    def quantize(self, value):\n",
    "        return min(self.qlevels, key=lambda x:abs(x-value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END-TO-END\n",
    "\n",
    "class HDC_Classifier():\n",
    "    def __init__(self, encoder, training_data=ISOLET()):\n",
    "        self.encoder = encoder\n",
    "        self.data = training_data\n",
    "        self.class_hvs = {}\n",
    "        self.training_encoded = {}\n",
    "        self.test_encoded = None\n",
    "\n",
    "    def train(self):      \n",
    "        # Group rows by class\n",
    "        classes = self.train_y().unique()\n",
    "        class_rows = {}\n",
    "        class_hvs = {}\n",
    "        \n",
    "        # Rows in each class\n",
    "        class_indexes = {}\n",
    "        for class_ in classes:\n",
    "            class_indexes[class_] = list(self.train_y()[self.train_y()==class_].index)\n",
    "\n",
    "        for class_ in classes:\n",
    "            class_rows[class_] = np.array(list(self.train_X().loc[class_indexes[class_]].itertuples(index=False, name=None)))\n",
    "        \n",
    "        encoded = {}\n",
    "        for class_ in classes:\n",
    "            print(\"Encoding... {}% \".format(round(100*class_/classes[-1],2)))\n",
    "            encoded[class_] = pd.Series(map(feature_encoder.encode, class_rows[class_]))\n",
    "        self.training_encoded = encoded\n",
    "        \n",
    "        accumulated = np.array([np.sum(encoded[class_]) for class_ in classes])\n",
    "        class_sparsifier = ThresholdingSparsifier(percent_max_val = 0.45, max_val=240)\n",
    "        thresholded = pd.Series(map(class_sparsifier.sparsify, accumulated))\n",
    "        thresholded.index = range(1,27)\n",
    "        \n",
    "        self.class_hvs = dict(thresholded)\n",
    "        \n",
    "        return \"Done\"\n",
    "    \n",
    "    def test(self):\n",
    "        encoded_test = pd.Series(map(self.encoder.encode, np.array(self.test_X())))\n",
    "        predictions = pd.Series(map(self.query, encoded_test))\n",
    "        return np.sum(predictions == self.test_y())/len(self.test_y())\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def query(self, query_hv):\n",
    "        d = dict([[class_, SparseHDC.dot(class_hv, query_hv)] for class_,class_hv in self.class_hvs.items()])\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    def train_X(self):\n",
    "        return self.data.train_X\n",
    "    \n",
    "    def train_y(self):\n",
    "        return self.data.train_y\n",
    "    \n",
    "    def test_X(self):\n",
    "        return self.data.test_X\n",
    "    \n",
    "    def test_y(self):\n",
    "        return self.data.test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIMIZATION RESULTS\n",
    "#Encoding from 168ms to 77.2ms @10k bits\n",
    "#Training time reduced from 80mins to 9mins\n",
    "\n",
    "#-68ms for binding <br>\n",
    "# ~-2ms for removing from function\n",
    "\n",
    "# 6.0ms for quantization\n",
    "\n",
    "# 0.5ms mapping\n",
    "\n",
    "# 0.2ms for sparsification\n",
    "#   ~-0.5ms by removing from function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL DEVELOPMENT\n",
    "\n",
    "### CONVERT NEXT THREE CELLS TO CODE AND RUN AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS ONCE\n",
    "\n",
    "isolet = ISOLET()\n",
    "\n",
    "# Rows in each class\n",
    "class_indexes = {}\n",
    "classes = range(1,27)\n",
    "for class_ in classes:\n",
    "    class_indexes[class_] = list(isolet.train_y[isolet.train_y==class_].index)\n",
    "\n",
    "# Rows for each class\n",
    "# class_rows[class_no][sample_no], class_no corresponds to A-Z but 1-26 instead\n",
    "class_rows = {}\n",
    "for class_ in classes:\n",
    "    class_rows[class_] = np.array(list(isolet.train_X.loc[class_indexes[class_]].itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE THE PARAMETERS HERE\n",
    "dim=10000\n",
    "sparsity=0.1\n",
    "\n",
    "cim_generator = LinearCIM(sparsity=sparsity, dim=dim)\n",
    "binder = AdditiveCDTBinder(sparsity=sparsity)\n",
    "sparsifier = ThresholdingSparsifier(percent_max_val=0.45)\n",
    "feature_encoder = Sparse_FeatureEncoder(\n",
    "                    cim_generator = cim_generator,\n",
    "                    binder = binder,\n",
    "                    sparsifier = sparsifier,\n",
    "                    sparsity = sparsity,\n",
    "                    dim = dim\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding... 3.85% \n",
      "Encoding... 7.69% \n",
      "Encoding... 11.54% \n",
      "Encoding... 15.38% \n",
      "Encoding... 19.23% \n",
      "Encoding... 23.08% \n",
      "Encoding... 26.92% \n",
      "Encoding... 30.77% \n",
      "Encoding... 34.62% \n",
      "Encoding... 38.46% \n",
      "Encoding... 42.31% \n",
      "Encoding... 46.15% \n",
      "Encoding... 50.0% \n",
      "Encoding... 53.85% \n",
      "Encoding... 57.69% \n",
      "Encoding... 61.54% \n",
      "Encoding... 65.38% \n",
      "Encoding... 69.23% \n",
      "Encoding... 73.08% \n",
      "Encoding... 76.92% \n",
      "Encoding... 80.77% \n",
      "Encoding... 84.62% \n",
      "Encoding... 88.46% \n",
      "Encoding... 92.31% \n",
      "Encoding... 96.15% \n",
      "Encoding... 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = HDC_Classifier(encoder = feature_encoder)\n",
    "classifier.train()\n",
    "# # Save models in the format \"HDC_Classifier_<CIM Method>_<Binding Method>_<Sparsifying Method>_S<sparsity>_D<dimensions>.pkl\"\n",
    "# # e.g. HDCModels.save_model(model=classifier, filename='HDC_Classifier_IM_BIND_TRESH_S0.x_Dxxxxx.pkl')\n",
    "# HDCModels.save_model(model=classifier, filename='HDC_Classifier_LCIM_BIND_TRESH_S0.3_D10000.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXISTING MODEL\n",
    "\n",
    "# classifier = HDCModels.load_model(filename='models/HDC_Classifier_LCIM_ACDT_TH_S0.3_D10000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11994868505452214"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.test() # FIX THREHSOLD FOR ACCUMULATED VECTORS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
